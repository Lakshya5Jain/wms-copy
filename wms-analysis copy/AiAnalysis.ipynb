{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "# Rest of your code stays exactly the same\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.colors import CSS4_COLORS, hsv_to_rgb\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import ast \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import plotly.express as px\n",
    "from datetime import timedelta, datetime\n",
    "import openai\n",
    "import time \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import qualitative, sequential\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import plotly.colors\n",
    "\n",
    "# Show ALL columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# set openai api key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-4.1\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset\n",
    "sku_data = pd.read_parquet('sku_data.parquet')\n",
    "\n",
    "#Inspect Datasets: Overview, Data Types, and Missing Values\n",
    "def inspect_dataset(df):\n",
    "    print(f\"\\n{df} Dataset Overview:\")\n",
    "    print(\"\\nHEAD:\\n\", df.head())\n",
    "    print(\"\\nINFO:\\n\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMISSING VALUES:\\n\", df.isnull().sum())\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "# Run inspections\n",
    "#df = sku_data.sample(n=20, random_state=None)\n",
    "\n",
    "# Convert date columns to datetime, handling missing values\n",
    "sku_data['created_date'] = pd.to_datetime(sku_data['created_date'], errors='coerce')\n",
    "sku_data['updated_date'] = pd.to_datetime(sku_data['updated_date'], errors='coerce')\n",
    "\n",
    "# Calculate the earliest and latest dates in the 'created_date' column\n",
    "earliest_created_date = sku_data['created_date'].min()\n",
    "latest_created_date = sku_data['created_date'].max()\n",
    "\n",
    "print(\"Earliest created_date:\", earliest_created_date)\n",
    "print(\"Latest created_date:\", latest_created_date)\n",
    "\n",
    "# Calculate the earliest and latest dates in the 'updated_date' column\n",
    "earliest_updated_date = sku_data['updated_date'].min()\n",
    "latest_updated_date = sku_data['updated_date'].max()\n",
    "\n",
    "print(\"Earliest updated_date:\", earliest_updated_date)\n",
    "print(\"Latest updated_date:\", latest_updated_date)\n",
    "\n",
    "# Set now to the latest updated_date\n",
    "now = latest_updated_date\n",
    "\n",
    "inspect_dataset(sku_data)\n",
    "sku_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cost_per_unit column with value 13.73 for all SKUs\n",
    "sku_data['cost_per_unit'] = 13.73\n",
    "\n",
    "sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with empty historical_inventory, historical_sales, and historical_received\n",
    "empty_inventory_sales_received = sku_data[\n",
    "    (sku_data['historical_inventory'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))) & \n",
    "    (sku_data['historical_sales'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))) & \n",
    "    (sku_data['historical_received'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x)))\n",
    "]\n",
    "\n",
    "# Print the rows that will be removed\n",
    "print(\"Rows with empty historical_inventory, historical_sales, and historical_received:\")\n",
    "print(empty_inventory_sales_received[['sku', 'name']])\n",
    "\n",
    "# Remove these rows from sku_data\n",
    "sku_data = sku_data[~((sku_data['historical_inventory'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))) & \n",
    "                      (sku_data['historical_sales'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))) & \n",
    "                      (sku_data['historical_received'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))))]\n",
    "\n",
    "empty_inventory = sku_data[\n",
    "    sku_data['historical_inventory'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))\n",
    "]\n",
    "\n",
    "# Print the rows that will be removed for historical_inventory\n",
    "print(\"\\n\\nRows with empty historical_inventory:\")\n",
    "print(empty_inventory[['sku', 'name']])\n",
    "print(\"\\n\\nNumber of rows with empty historical_inventory:\", empty_inventory.shape[0])\n",
    "\n",
    "\n",
    "empty_sales = sku_data[\n",
    "    sku_data['historical_sales'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))\n",
    "]\n",
    "\n",
    "# Print the rows that will be removed for historical_sales\n",
    "print(\"\\n\\nRows with empty historical_sales:\")\n",
    "print(empty_sales[['sku', 'name']])\n",
    "print(\"\\n\\nNumber of rows with empty historical_sales:\", empty_sales.shape[0])\n",
    "empty_received = sku_data[\n",
    "    sku_data['historical_received'].apply(lambda x: len(x) == 0 if isinstance(x, (list, np.ndarray)) else pd.isnull(x))\n",
    "]\n",
    "\n",
    "# Print the rows that will be removed for historical_received\n",
    "print(\"\\n\\nRows with empty historical_received:\")\n",
    "print(empty_received[['sku', 'name']])\n",
    "print(\"\\n\\nNumber of rows with empty historical_received:\", empty_received.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_data = sku_data.reset_index(drop=True)\n",
    "inspect_dataset(sku_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added inventory collums \n",
    "# current_inventory, inventory_30d_ago, inventory_90d_ago, inventory_180d_ago, inventory_360d_ago, initial_inventory\n",
    "\n",
    "def get_current_inventory(historical_inventory):\n",
    "    \"\"\"Get the most recent inventory value\"\"\"\n",
    "    # Handle both empty lists and empty arrays\n",
    "    if isinstance(historical_inventory, (list, np.ndarray)) and len(historical_inventory) == 0:\n",
    "        return np.nan\n",
    "    # Handle pandas Series or other array-like objects\n",
    "    try:\n",
    "        if len(historical_inventory) == 0:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    value = historical_inventory[-1]['inventory']\n",
    "    return float(value) if not pd.isna(value) else np.nan\n",
    "\n",
    "def get_inventory_before_date(historical_inventory, target_date):\n",
    "    \"\"\"Find inventory closest to but before a target date\"\"\"\n",
    "    # Handle empty data\n",
    "    if isinstance(historical_inventory, (list, np.ndarray)) and len(historical_inventory) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        if len(historical_inventory) == 0:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert target_date to pandas Timestamp for consistent comparison\n",
    "    if hasattr(target_date, 'date') and not hasattr(target_date, 'hour'):\n",
    "        # It's a date object, convert to pandas Timestamp\n",
    "        target_date = pd.Timestamp(target_date)\n",
    "    elif not isinstance(target_date, pd.Timestamp):\n",
    "        target_date = pd.Timestamp(target_date)\n",
    "    \n",
    "    # Make target_date timezone-naive if it has timezone info\n",
    "    if hasattr(target_date, 'tzinfo') and target_date.tzinfo is not None:\n",
    "        target_date = target_date.tz_localize(None)\n",
    "    \n",
    "    # Filter entries before or equal to target_date\n",
    "    filtered = []\n",
    "    for entry in historical_inventory:\n",
    "        entry_date = entry['date']\n",
    "        \n",
    "        # Convert string dates to pandas Timestamp\n",
    "        if isinstance(entry_date, str):\n",
    "            try:\n",
    "                entry_date = pd.Timestamp(entry_date)\n",
    "            except:\n",
    "                continue  # Skip entries with invalid date strings\n",
    "        \n",
    "        # Convert date objects to pandas Timestamp for comparison\n",
    "        if hasattr(entry_date, 'date') and not hasattr(entry_date, 'hour'):\n",
    "            # It's a date object, convert to pandas Timestamp\n",
    "            entry_date = pd.Timestamp(entry_date)\n",
    "        elif not isinstance(entry_date, pd.Timestamp):\n",
    "            entry_date = pd.Timestamp(entry_date)\n",
    "        \n",
    "        # Make entry_date timezone-naive if it has timezone info\n",
    "        if hasattr(entry_date, 'tzinfo') and entry_date.tzinfo is not None:\n",
    "            entry_date = entry_date.tz_localize(None)\n",
    "        \n",
    "        if entry_date <= target_date:\n",
    "            filtered.append(entry)\n",
    "    \n",
    "    if len(filtered) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    # Return inventory of the last entry before or on target_date\n",
    "    value = filtered[-1]['inventory']\n",
    "    return float(value) if not pd.isna(value) else np.nan\n",
    "\n",
    "def get_inventory_days_ago(historical_inventory, reference_date, days_ago):\n",
    "    \"\"\"Calculate inventory N days ago from reference date\"\"\"\n",
    "    target_date = reference_date - timedelta(days=days_ago)\n",
    "    return get_inventory_before_date(historical_inventory, target_date)\n",
    "\n",
    "def get_initial_inventory(historical_inventory):\n",
    "    \"\"\"Calculate initial inventory by going back one step from first entry\"\"\"\n",
    "    # Handle empty data\n",
    "    if isinstance(historical_inventory, (list, np.ndarray)) and len(historical_inventory) == 0:\n",
    "        return np.nan\n",
    "    try:\n",
    "        if len(historical_inventory) == 0:\n",
    "            return np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "    first_entry = historical_inventory[0]\n",
    "    # Initial inventory = first recorded inventory - quantity_change\n",
    "    # This gives us the inventory before the first transaction\n",
    "    initial_inventory = first_entry['inventory'] - first_entry['net_change']\n",
    "    return float(initial_inventory) if not pd.isna(initial_inventory) else np.nan\n",
    "\n",
    "# Calculate days_on_market first\n",
    "sku_data['days_on_market'] = (sku_data['updated_date'] - sku_data['created_date']).dt.days\n",
    "\n",
    "# Apply these functions to your dataframe\n",
    "sku_data['current_inventory'] = sku_data['historical_inventory'].apply(get_current_inventory)\n",
    "sku_data['inventory_30d_ago'] = sku_data['historical_inventory'].apply(\n",
    "    lambda x: get_inventory_days_ago(x, now, 30)\n",
    ")\n",
    "sku_data['inventory_90d_ago'] = sku_data['historical_inventory'].apply(\n",
    "    lambda x: get_inventory_days_ago(x, now, 90)\n",
    ")\n",
    "sku_data['inventory_180d_ago'] = sku_data['historical_inventory'].apply(\n",
    "    lambda x: get_inventory_days_ago(x, now, 180)\n",
    ")\n",
    "sku_data['inventory_360d_ago'] = sku_data['historical_inventory'].apply(\n",
    "    lambda x: get_inventory_days_ago(x, now, 360)\n",
    ")\n",
    "sku_data['initial_inventory'] = sku_data['historical_inventory'].apply(get_initial_inventory)\n",
    "\n",
    "# Test if target dates are before created dates but still have non-null values\n",
    "invalid_skus_30d = []\n",
    "invalid_skus_90d = []\n",
    "invalid_skus_180d = []\n",
    "invalid_skus_360d = []\n",
    "\n",
    "for idx, row in sku_data.iterrows():\n",
    "    target_date_30d = now - timedelta(days=30)\n",
    "    target_date_90d = now - timedelta(days=90)\n",
    "    target_date_180d = now - timedelta(days=180)\n",
    "    target_date_360d = now - timedelta(days=360)\n",
    "    \n",
    "    if target_date_30d < row['created_date'] and not pd.isna(row['inventory_30d_ago']):\n",
    "        invalid_skus_30d.append(row['sku'])\n",
    "    if target_date_90d < row['created_date'] and not pd.isna(row['inventory_90d_ago']):\n",
    "        invalid_skus_90d.append(row['sku'])\n",
    "    if target_date_180d < row['created_date'] and not pd.isna(row['inventory_180d_ago']):\n",
    "        invalid_skus_180d.append(row['sku'])\n",
    "    if target_date_360d < row['created_date'] and not pd.isna(row['inventory_360d_ago']):\n",
    "        invalid_skus_360d.append(row['sku'])\n",
    "\n",
    "print(\"SKUs with invalid 30d inventory values:\", invalid_skus_30d)\n",
    "print(\"SKUs with invalid 90d inventory values:\", invalid_skus_90d)\n",
    "print(\"SKUs with invalid 180d inventory values:\", invalid_skus_180d)\n",
    "print(\"SKUs with invalid 360d inventory values:\", invalid_skus_360d)\n",
    "\n",
    "# Reorder columns to place days_on_market after updated_date\n",
    "cols = list(sku_data.columns)\n",
    "updated_date_idx = cols.index('updated_date')\n",
    "days_on_market_idx = cols.index('days_on_market')\n",
    "\n",
    "# Remove days_on_market from current position\n",
    "cols.pop(days_on_market_idx)\n",
    "# Insert it right after updated_date\n",
    "cols.insert(updated_date_idx + 1, 'days_on_market')\n",
    "\n",
    "# Reorder the dataframe\n",
    "sku_data = sku_data[cols]\n",
    "\n",
    "sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop specific SKUs: LGBOX, MDBOX, SMBOX\n",
    "skus_to_drop = ['LGBOX', 'MDBOX', 'SMBOX']\n",
    "# Show rows before dropping them\n",
    "rows_to_drop = sku_data[sku_data['sku'].isin(skus_to_drop)]\n",
    "print(f\"Rows to be dropped ({len(rows_to_drop)} SKUs):\")\n",
    "print(rows_to_drop)\n",
    "\n",
    "sku_data = sku_data[~sku_data['sku'].isin(skus_to_drop)]\n",
    "print(f\"Dropped {len(skus_to_drop)} SKUs: {skus_to_drop}\")\n",
    "\n",
    "# Find all rows with negative initial inventory\n",
    "negative_initial_inventory = sku_data[sku_data['initial_inventory'] < -100]\n",
    "print(f\"Found {len(negative_initial_inventory)} SKUs with negative initial inventory:\")\n",
    "negative_initial_inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added recived inventory info\n",
    "#recived_inventory_30d_ago, recived_inventory_90d_ago, recived_inventory_180d_ago, recived_inventory_360d_ago, recived_inventory\n",
    "\n",
    "def get_total_received_inventory(historical_received):\n",
    "    \"\"\"Calculate total received inventory from historical_received list\"\"\"\n",
    "    if isinstance(historical_received, (list, np.ndarray)) and len(historical_received) == 0:\n",
    "        return 0.0  # Changed np.nan to 0.0\n",
    "    try:\n",
    "        if len(historical_received) == 0:\n",
    "            return 0.0  # Changed np.nan to 0.0\n",
    "    except:\n",
    "        return 0.0  # Changed np.nan to 0.0\n",
    "    \n",
    "    total_received = 0.0\n",
    "    for entry in historical_received:\n",
    "        total_received += float(entry.get('quantity', 0))\n",
    "    return total_received\n",
    "\n",
    "def get_received_inventory_in_period(historical_received, reference_date, days_ago):\n",
    "    \"\"\"Calculate total received inventory in the period from days_ago to now\"\"\"\n",
    "    if isinstance(historical_received, (list, np.ndarray)) and len(historical_received) == 0:\n",
    "        return 0.0\n",
    "    try:\n",
    "        if len(historical_received) == 0:\n",
    "            return 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate the start date of the period (days_ago from reference_date)\n",
    "    start_date = reference_date - timedelta(days=days_ago)\n",
    "    \n",
    "    # Convert all dates to date objects for consistent comparison\n",
    "    if hasattr(reference_date, 'date'):\n",
    "        reference_date = reference_date.date()\n",
    "    if hasattr(start_date, 'date'):\n",
    "        start_date = start_date.date()\n",
    "    \n",
    "    total_received = 0.0\n",
    "    for entry in historical_received:\n",
    "        entry_date = entry['date']\n",
    "        # Convert entry date to date object for consistent comparison\n",
    "        if hasattr(entry_date, 'date'):\n",
    "            entry_date = entry_date.date()\n",
    "        \n",
    "        # Check if entry date is within the period (start_date < entry_date <= reference_date)\n",
    "        if start_date < entry_date <= reference_date:\n",
    "            total_received += float(entry.get('quantity', 0))\n",
    "    \n",
    "    return total_received\n",
    "\n",
    "# Create a copy of the dataframe to avoid SettingWithCopyWarning\n",
    "sku_data = sku_data.copy()\n",
    "\n",
    "# Apply received inventory functions to your dataframe\n",
    "sku_data['received_inventory_30d_ago'] = sku_data['historical_received'].apply(\n",
    "    lambda x: get_received_inventory_in_period(x, now, 30)\n",
    ").astype('float64')\n",
    "sku_data['received_inventory_90d_ago'] = sku_data['historical_received'].apply(\n",
    "    lambda x: get_received_inventory_in_period(x, now, 90)\n",
    ").astype('float64')\n",
    "sku_data['received_inventory_180d_ago'] = sku_data['historical_received'].apply(\n",
    "    lambda x: get_received_inventory_in_period(x, now, 180)\n",
    ").astype('float64')\n",
    "sku_data['received_inventory_360d_ago'] = sku_data['historical_received'].apply(\n",
    "    lambda x: get_received_inventory_in_period(x, now, 360)\n",
    ").astype('float64')\n",
    "sku_data['received_inventory'] = sku_data['historical_received'].apply(get_total_received_inventory).astype('float64')\n",
    "\n",
    "sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataset(sku_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added sales info\n",
    "# total_sales, sales_last_90d, sales_last_180d, sales_last_360d\n",
    "\n",
    "def get_total_sales(historical_sales):\n",
    "    \"\"\"Calculate total quantity sold from historical_sales list\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    total_sold = 0\n",
    "    for entry in historical_sales:\n",
    "        total_sold += entry.get('quantity', 0)\n",
    "    return total_sold\n",
    "\n",
    "def get_sales_before_date(historical_sales, target_date):\n",
    "    \"\"\"Calculate total sales up to and including target_date\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    # Convert target_date to date object for consistent comparison\n",
    "    if hasattr(target_date, 'date'):\n",
    "        target_date = target_date.date()\n",
    "    \n",
    "    total_sold = 0\n",
    "    for entry in historical_sales:\n",
    "        entry_date = entry['date']\n",
    "        # Convert entry date to date object for consistent comparison\n",
    "        if hasattr(entry_date, 'date'):\n",
    "            entry_date = entry_date.date()\n",
    "        \n",
    "        if entry_date <= target_date:\n",
    "            total_sold += entry.get('quantity', 0)\n",
    "    \n",
    "    return total_sold\n",
    "\n",
    "def get_sales_days_ago(historical_sales, reference_date, days_ago):\n",
    "    \"\"\"Calculate sales N days ago from reference date\"\"\"\n",
    "    target_date = reference_date - timedelta(days=days_ago)\n",
    "    return get_sales_before_date(historical_sales, target_date)\n",
    "\n",
    "# Apply sales functions to your dataframe\n",
    "sku_data['sales_last_30d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_sales_days_ago(x, now, 0) - get_sales_days_ago(x, now, 30)\n",
    ")\n",
    "sku_data['sales_last_90d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_sales_days_ago(x, now, 0) - get_sales_days_ago(x, now, 90)\n",
    ")\n",
    "sku_data['sales_last_180d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_sales_days_ago(x, now, 0) - get_sales_days_ago(x, now, 180)\n",
    ")\n",
    "sku_data['sales_last_360d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_sales_days_ago(x, now, 0) - get_sales_days_ago(x, now, 360)\n",
    ")\n",
    "sku_data['total_sales'] = sku_data['historical_sales'].apply(get_total_sales)\n",
    "\n",
    "sku_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added avg_unit_price, min_unit_price, max_unit_price, total_revenue, revenue_last_30d, revenue_last_60d, revenue_last_90d, revenue_last_180d, \n",
    "# revenue_last_360d, avg_daily_revenue_30d, avg_daily_revenue_60d, avg_daily_revenue_90d, avg_daily_revenue_180d, avg_daily_revenue_360d\n",
    "\n",
    "\n",
    "def get_avg_unit_price(historical_sales):\n",
    "    \"\"\"Calculate quantity-weighted average unit price from historical_sales list\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    total_revenue = 0\n",
    "    total_quantity = 0\n",
    "    \n",
    "    for entry in historical_sales:\n",
    "        unit_price = entry.get('unit_price', None)  # Changed from 'price' to 'unit_price'\n",
    "        quantity = entry.get('quantity', 0)\n",
    "        \n",
    "        if unit_price is not None and quantity > 0:\n",
    "            total_revenue += unit_price * quantity\n",
    "            total_quantity += quantity\n",
    "    \n",
    "    if total_quantity == 0:\n",
    "        return 0  \n",
    "    \n",
    "    return total_revenue / total_quantity\n",
    "\n",
    "def get_avg_unit_price_last_n_days(historical_sales, reference_date, days):\n",
    "    \"\"\"Calculate quantity-weighted average unit price for the last N days from reference date\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    # Convert reference_date to date object for consistent comparison\n",
    "    if hasattr(reference_date, 'date'):\n",
    "        reference_date = reference_date.date()\n",
    "    \n",
    "    cutoff_date = reference_date - timedelta(days=days)\n",
    "    \n",
    "    total_revenue = 0\n",
    "    total_quantity = 0\n",
    "    \n",
    "    for entry in historical_sales:\n",
    "        if isinstance(entry, dict) and 'date' in entry and 'unit_price' in entry and 'quantity' in entry:\n",
    "            entry_date = entry['date']\n",
    "            # Convert entry date to date object for consistent comparison\n",
    "            if hasattr(entry_date, 'date'):\n",
    "                entry_date = entry_date.date()\n",
    "            \n",
    "            if entry_date > cutoff_date and entry_date <= reference_date:\n",
    "                unit_price = entry.get('unit_price', None)\n",
    "                quantity = entry.get('quantity', 0)\n",
    "                \n",
    "                if unit_price is not None and quantity > 0:\n",
    "                    total_revenue += unit_price * quantity\n",
    "                    total_quantity += quantity\n",
    "    \n",
    "    if total_quantity == 0:\n",
    "        return 0\n",
    "    \n",
    "    return total_revenue / total_quantity\n",
    "\n",
    "\n",
    "def get_min_max_unit_price(historical_sales):\n",
    "    \"\"\"Extract min and max unit price from historical_sales\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return {'min_unit_price': 0.0, 'max_unit_price': 0.0}\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return {'min_unit_price': 0.0, 'max_unit_price': 0.0}\n",
    "    except:\n",
    "        return {'min_unit_price': 0.0, 'max_unit_price': 0.0}\n",
    "    \n",
    "    unit_prices = []\n",
    "    for entry in historical_sales:\n",
    "        if isinstance(entry, dict) and 'unit_price' in entry:\n",
    "            unit_price = entry.get('unit_price')\n",
    "            if unit_price is not None and unit_price > 0:\n",
    "                unit_prices.append(unit_price)\n",
    "    \n",
    "    if len(unit_prices) == 0:\n",
    "        return {'min_unit_price': 0.0, 'max_unit_price': 0.0}\n",
    "    \n",
    "    return {\n",
    "        'min_unit_price': min(unit_prices),\n",
    "        'max_unit_price': max(unit_prices)\n",
    "    }\n",
    "\n",
    "def get_total_revenue(historical_sales):\n",
    "    \"\"\"Calculate total revenue from historical_sales\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    total_revenue = 0\n",
    "    for entry in historical_sales:\n",
    "        if isinstance(entry, dict) and 'total_value' in entry:\n",
    "            total_value = entry.get('total_value', 0)\n",
    "            if total_value is not None:\n",
    "                total_revenue += total_value\n",
    "    \n",
    "    return total_revenue\n",
    "\n",
    "def get_revenue_last_n_days(historical_sales, reference_date, days):\n",
    "    \"\"\"Calculate total revenue for the last N days from reference date\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return 0\n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "    # Convert reference_date to date object for consistent comparison\n",
    "    if hasattr(reference_date, 'date'):\n",
    "        reference_date = reference_date.date()\n",
    "    \n",
    "    cutoff_date = reference_date - timedelta(days=days)\n",
    "    \n",
    "    revenue_in_period = 0\n",
    "    for entry in historical_sales:\n",
    "        if isinstance(entry, dict) and 'date' in entry and 'total_value' in entry:\n",
    "            entry_date = entry['date']\n",
    "            # Convert entry date to date object for consistent comparison\n",
    "            if hasattr(entry_date, 'date'):\n",
    "                entry_date = entry_date.date()\n",
    "            \n",
    "            if entry_date > cutoff_date and entry_date <= reference_date:\n",
    "                total_value = entry.get('total_value', 0)\n",
    "                if total_value is not None:\n",
    "                    revenue_in_period += total_value\n",
    "    \n",
    "    return revenue_in_period\n",
    "\n",
    "# Apply average unit price calculation \n",
    "sku_data['avg_unit_price'] = sku_data['historical_sales'].apply(get_avg_unit_price)\n",
    "\n",
    "\n",
    "# Apply min/max unit price calculations\n",
    "price_stats = sku_data['historical_sales'].apply(get_min_max_unit_price)\n",
    "sku_data['min_unit_price'] = price_stats.apply(lambda x: x['min_unit_price'])\n",
    "sku_data['max_unit_price'] = price_stats.apply(lambda x: x['max_unit_price'])\n",
    "\n",
    "# Apply total revenue calculation\n",
    "sku_data['total_revenue'] = sku_data['historical_sales'].apply(get_total_revenue)\n",
    "\n",
    "# Apply revenue calculations for different time periods\n",
    "sku_data['revenue_last_30d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_revenue_last_n_days(x, now, 30)\n",
    ")\n",
    "sku_data['revenue_last_60d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_revenue_last_n_days(x, now, 60)\n",
    ")\n",
    "sku_data['revenue_last_90d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_revenue_last_n_days(x, now, 90)\n",
    ")\n",
    "sku_data['revenue_last_180d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_revenue_last_n_days(x, now, 180)\n",
    ")\n",
    "sku_data['revenue_last_360d'] = sku_data['historical_sales'].apply(\n",
    "    lambda x: get_revenue_last_n_days(x, now, 360)\n",
    ")\n",
    "\n",
    "# Calculate average daily revenue for each period\n",
    "sku_data['avg_daily_revenue_30d'] = sku_data['revenue_last_30d'] / 30\n",
    "sku_data['avg_daily_revenue_60d'] = sku_data['revenue_last_60d'] / 60\n",
    "sku_data['avg_daily_revenue_90d'] = sku_data['revenue_last_90d'] / 90\n",
    "sku_data['avg_daily_revenue_180d'] = sku_data['revenue_last_180d'] / 180\n",
    "sku_data['avg_daily_revenue_360d'] = sku_data['revenue_last_360d'] / 360\n",
    "\n",
    "# Display the new columns\n",
    "sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic velocity calculation\n",
    "sku_data['velocity'] = sku_data['total_sales'] / sku_data['days_on_market'].replace(0, np.nan)\n",
    "\n",
    "# Calculate velocity for different time periods (units per day)\n",
    "sku_data['velocity_last_30d'] = sku_data['sales_last_30d'] / 30\n",
    "sku_data['velocity_last_90d'] = sku_data['sales_last_90d'] / 90\n",
    "sku_data['velocity_last_180d'] = sku_data['sales_last_180d'] / 180\n",
    "sku_data['velocity_last_360d'] = sku_data['sales_last_360d'] / 360\n",
    "\n",
    "# Function to calculate rolling 30-day velocity statistics with improved error handling\n",
    "def calculate_velocity_stats(historical_sales):\n",
    "    \"\"\"Calculate max/min 30-day velocity and daily sales stats\"\"\"\n",
    "    if isinstance(historical_sales, (list, np.ndarray)) and len(historical_sales) == 0:\n",
    "        return {\n",
    "            'max_30d_velocity_value': np.nan,\n",
    "            'max_30d_velocity_range_start': np.nan,\n",
    "            'max_30d_velocity_range_end': np.nan,\n",
    "            'min_30d_velocity_value': np.nan,\n",
    "            'min_30d_velocity_range_start': np.nan,\n",
    "            'min_30d_velocity_range_end': np.nan,\n",
    "            'max_daily_sales_value': 0,\n",
    "            'max_daily_sales_date': np.nan,\n",
    "            'min_daily_sales_value': 0,\n",
    "            'min_daily_sales_date': np.nan\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        if len(historical_sales) == 0:\n",
    "            return {\n",
    "                'max_30d_velocity_value': np.nan,\n",
    "                'max_30d_velocity_range_start': np.nan,\n",
    "                'max_30d_velocity_range_end': np.nan,\n",
    "                'min_30d_velocity_value': np.nan,\n",
    "                'min_30d_velocity_range_start': np.nan,\n",
    "                'min_30d_velocity_range_end': np.nan,\n",
    "                'max_daily_sales_value': 0,\n",
    "                'max_daily_sales_date': np.nan,\n",
    "                'min_daily_sales_value': 0,\n",
    "                'min_daily_sales_date': np.nan\n",
    "            }\n",
    "    except:\n",
    "        return {\n",
    "            'max_30d_velocity_value': np.nan,\n",
    "            'max_30d_velocity_range_start': np.nan,\n",
    "            'max_30d_velocity_range_end': np.nan,\n",
    "            'min_30d_velocity_value': np.nan,\n",
    "            'min_30d_velocity_range_start': np.nan,\n",
    "            'min_30d_velocity_range_end': np.nan,\n",
    "            'max_daily_sales_value': 0,\n",
    "            'max_daily_sales_date': np.nan,\n",
    "            'min_daily_sales_value': 0,\n",
    "            'min_daily_sales_date': np.nan\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Filter entries that have both 'date' and 'quantity' keys\n",
    "        valid_entries = []\n",
    "        for entry in historical_sales:\n",
    "            if isinstance(entry, dict) and 'date' in entry and 'quantity' in entry:\n",
    "                valid_entries.append(entry)\n",
    "        \n",
    "        if len(valid_entries) == 0:\n",
    "            return {\n",
    "                'max_30d_velocity_value': np.nan,\n",
    "                'max_30d_velocity_range_start': np.nan,\n",
    "                'max_30d_velocity_range_end': np.nan,\n",
    "                'min_30d_velocity_value': np.nan,\n",
    "                'min_30d_velocity_range_start': np.nan,\n",
    "                'min_30d_velocity_range_end': np.nan,\n",
    "                'max_daily_sales_value': 0,\n",
    "                'max_daily_sales_date': np.nan,\n",
    "                'min_daily_sales_value': 0,\n",
    "                'min_daily_sales_date': np.nan\n",
    "            }\n",
    "        \n",
    "        # Convert to DataFrame and process\n",
    "        sales_df = pd.DataFrame(valid_entries)\n",
    "        sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "        \n",
    "        # Handle timezone conversion\n",
    "        if sales_df['date'].dt.tz is not None:\n",
    "            sales_df['date'] = sales_df['date'].dt.tz_convert(None)\n",
    "        \n",
    "        sales_df = sales_df.sort_values('date')\n",
    "        \n",
    "        # Create date range and aggregate sales by date\n",
    "        start_date = sales_df['date'].min()\n",
    "        end_date = sales_df['date'].max()\n",
    "        \n",
    "        # If date range is less than 30 days, return NaN for velocity stats\n",
    "        if (end_date - start_date).days < 30:\n",
    "            # Still calculate daily stats\n",
    "            sales_by_date = sales_df.groupby('date')['quantity'].sum()\n",
    "            max_daily = sales_by_date.max()\n",
    "            max_daily_date = sales_by_date.idxmax()\n",
    "            min_daily = sales_by_date.min()\n",
    "            min_daily_date = sales_by_date.idxmin()\n",
    "            \n",
    "            return {\n",
    "                'max_30d_velocity_value': np.nan,\n",
    "                'max_30d_velocity_range_start': np.nan,\n",
    "                'max_30d_velocity_range_end': np.nan,\n",
    "                'min_30d_velocity_value': np.nan,\n",
    "                'min_30d_velocity_range_start': np.nan,\n",
    "                'min_30d_velocity_range_end': np.nan,\n",
    "                'max_daily_sales_value': max_daily,\n",
    "                'max_daily_sales_date': max_daily_date,\n",
    "                'min_daily_sales_value': min_daily,\n",
    "                'min_daily_sales_date': min_daily_date\n",
    "            }\n",
    "        \n",
    "        date_range = pd.date_range(start=start_date, end=end_date)\n",
    "        sales_by_date = sales_df.groupby('date')['quantity'].sum().reindex(date_range, fill_value=0)\n",
    "        \n",
    "        # Calculate rolling 30-day velocity\n",
    "        rolling_30d_sales = sales_by_date.rolling(window=30).sum()\n",
    "        rolling_30d_velocity = rolling_30d_sales / 30\n",
    "        \n",
    "        # Find max and min 30-day velocity\n",
    "        max_30d_velocity_value = rolling_30d_velocity.max()\n",
    "        max_30d_velocity_date = rolling_30d_velocity.idxmax()\n",
    "        min_30d_velocity_value = rolling_30d_velocity.min()\n",
    "        min_30d_velocity_date = rolling_30d_velocity.idxmin()\n",
    "        \n",
    "        # Calculate date ranges\n",
    "        max_30d_velocity_range_start = max_30d_velocity_date - pd.Timedelta(days=29)\n",
    "        max_30d_velocity_range_end = max_30d_velocity_date\n",
    "        min_30d_velocity_range_start = min_30d_velocity_date - pd.Timedelta(days=29)\n",
    "        min_30d_velocity_range_end = min_30d_velocity_date\n",
    "        \n",
    "        # Find max and min daily sales\n",
    "        max_daily_sales_value = sales_by_date.max()\n",
    "        max_daily_sales_date = sales_by_date.idxmax()\n",
    "        min_daily_sales_value = sales_by_date.min()\n",
    "        min_daily_sales_date = sales_by_date.idxmin()\n",
    "        \n",
    "        return {\n",
    "            'max_30d_velocity_value': max_30d_velocity_value,\n",
    "            'max_30d_velocity_range_start': max_30d_velocity_range_start,\n",
    "            'max_30d_velocity_range_end': max_30d_velocity_range_end,\n",
    "            'min_30d_velocity_value': min_30d_velocity_value,\n",
    "            'min_30d_velocity_range_start': min_30d_velocity_range_start,\n",
    "            'min_30d_velocity_range_end': min_30d_velocity_range_end,\n",
    "            'max_daily_sales_value': max_daily_sales_value,\n",
    "            'max_daily_sales_date': max_daily_sales_date,\n",
    "            'min_daily_sales_value': min_daily_sales_value,\n",
    "            'min_daily_sales_date': min_daily_sales_date\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing historical_sales: {e}\")\n",
    "        return {\n",
    "            'max_30d_velocity_value': np.nan,\n",
    "            'max_30d_velocity_range_start': np.nan,\n",
    "            'max_30d_velocity_range_end': np.nan,\n",
    "            'min_30d_velocity_value': np.nan,\n",
    "            'min_30d_velocity_range_start': np.nan,\n",
    "            'min_30d_velocity_range_end': np.nan,\n",
    "            'max_daily_sales_value': np.nan,\n",
    "            'max_daily_sales_date': np.nan,\n",
    "            'min_daily_sales_value': np.nan,\n",
    "            'min_daily_sales_date': np.nan\n",
    "        }\n",
    "\n",
    "# Apply velocity statistics calculation\n",
    "velocity_stats = sku_data['historical_sales'].apply(calculate_velocity_stats)\n",
    "\n",
    "# Extract individual columns from the velocity stats\n",
    "sku_data['max_30d_velocity_value'] = velocity_stats.apply(lambda x: x['max_30d_velocity_value'])\n",
    "sku_data['max_30d_velocity_range_start'] = velocity_stats.apply(lambda x: x['max_30d_velocity_range_start'])\n",
    "sku_data['max_30d_velocity_range_end'] = velocity_stats.apply(lambda x: x['max_30d_velocity_range_end'])\n",
    "sku_data['min_30d_velocity_value'] = velocity_stats.apply(lambda x: x['min_30d_velocity_value'])\n",
    "sku_data['min_30d_velocity_range_start'] = velocity_stats.apply(lambda x: x['min_30d_velocity_range_start'])\n",
    "sku_data['min_30d_velocity_range_end'] = velocity_stats.apply(lambda x: x['min_30d_velocity_range_end'])\n",
    "sku_data['max_daily_sales_value'] = velocity_stats.apply(lambda x: x['max_daily_sales_value'])\n",
    "sku_data['max_daily_sales_date'] = velocity_stats.apply(lambda x: x['max_daily_sales_date'])\n",
    "sku_data['min_daily_sales_value'] = velocity_stats.apply(lambda x: x['min_daily_sales_value'])\n",
    "sku_data['min_daily_sales_date'] = velocity_stats.apply(lambda x: x['min_daily_sales_date'])\n",
    "\n",
    "sku_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataset(sku_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVENTORY AGE AND TURNOVER METRICS\n",
    "\n",
    "def days_in_inventory(current_inventory, velocity_last_30d):\n",
    "    \"\"\"\n",
    "    Calculate Days Sales in Inventory using recent 30-day velocity\n",
    "    \n",
    "    This metric tells you how many days it would take to sell all current inventory\n",
    "    at the current sales pace (based on last 30 days of sales activity).\n",
    "    \n",
    "    Formula: DSI = Current Inventory / Daily Sales Velocity (last 30 days)\n",
    "    \n",
    "    Parameters:\n",
    "    - current_inventory: Units currently in stock\n",
    "    - velocity_last_30d: Average daily sales rate over last 30 days (units/day)\n",
    "    \n",
    "    Returns:\n",
    "    - Number of days to sell current inventory at recent velocity\n",
    "    - NaN if no recent sales velocity or no inventory\n",
    "    \"\"\"\n",
    "    # Handle edge cases\n",
    "    if pd.isna(current_inventory) or current_inventory == 0:\n",
    "        return 0  # No inventory means 0 days to sell\n",
    "    \n",
    "    if pd.isna(velocity_last_30d) or velocity_last_30d == 0:\n",
    "        return np.nan  # Can't calculate if no recent sales\n",
    "    \n",
    "    # DSI = Current Inventory / Recent Daily Sales Rate\n",
    "    return current_inventory / velocity_last_30d\n",
    "\n",
    "\n",
    "def calculate_inventory_turnover(total_sales, current_inventory, initial_inventory):\n",
    "    \"\"\"\n",
    "    Calculate inventory turnover rate\n",
    "    \n",
    "    This measures how efficiently inventory is being sold by comparing total sales\n",
    "    to average inventory levels. Higher values indicate better inventory efficiency.\n",
    "    \n",
    "    Formula: Inventory Turnover = Total Sales / Average Inventory\n",
    "    where Average Inventory = (Current Inventory + Initial Inventory) / 2\n",
    "    \n",
    "    Parameters:\n",
    "    - total_sales: Total units sold over product lifetime\n",
    "    - current_inventory: Current units in stock\n",
    "    - initial_inventory: Starting inventory when product was first stocked\n",
    "    \n",
    "    Returns:\n",
    "    - Turnover ratio (times inventory was \"turned over\")\n",
    "    - Higher values = more efficient inventory management\n",
    "    \"\"\"\n",
    "    # Calculate average inventory level\n",
    "    if pd.isna(initial_inventory) or initial_inventory == 0:\n",
    "        # If no initial inventory data, use current inventory as proxy\n",
    "        average_inventory = current_inventory\n",
    "    else:\n",
    "        # Use midpoint between initial and current inventory\n",
    "        average_inventory = (current_inventory + initial_inventory) / 2\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if pd.isna(average_inventory) or average_inventory == 0:\n",
    "        return np.nan  # Can't calculate turnover with no inventory\n",
    "    \n",
    "    if pd.isna(total_sales) or total_sales == 0:\n",
    "        return 0  # No sales means zero turnover\n",
    "    \n",
    "    # Inventory Turnover = Total Sales / Average Inventory\n",
    "    return total_sales / average_inventory\n",
    "\n",
    "\n",
    "\n",
    "# APPLY METRICS TO DATAFRAME\n",
    "\n",
    "print(\"Calculating inventory age and turnover metrics...\")\n",
    "\n",
    "# Calculate Days Sales in Inventory using recent 30-day velocity\n",
    "# This tells us how long current inventory will last at current sales pace\n",
    "sku_data['days_sales_in_inventory'] = sku_data.apply(\n",
    "    lambda row: days_in_inventory(\n",
    "        row['current_inventory'], \n",
    "        row['velocity_last_30d']\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate Inventory Turnover Rate\n",
    "# This measures how efficiently we're converting inventory to sales\n",
    "sku_data['inventory_turnover'] = sku_data.apply(\n",
    "    lambda row: calculate_inventory_turnover(\n",
    "        row['total_sales'], \n",
    "        row['current_inventory'], \n",
    "        row['initial_inventory']\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"✓ Inventory metrics calculated successfully!\")\n",
    "\n",
    "\n",
    "# SUMMARY STATISTICS AND INSIGHTS\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INVENTORY METRICS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Days Sales in Inventory Statistics\n",
    "print(\"\\nDays Sales in Inventory (DSI) - Based on Last 30 Days Velocity:\")\n",
    "print(f\"  Mean DSI: {sku_data['days_sales_in_inventory'].mean():.1f} days\")\n",
    "print(f\"  Median DSI: {sku_data['days_sales_in_inventory'].median():.1f} days\")\n",
    "print(f\"  75th percentile: {sku_data['days_sales_in_inventory'].quantile(0.75):.1f} days\")\n",
    "print(f\"  90th percentile: {sku_data['days_sales_in_inventory'].quantile(0.90):.1f} days\")\n",
    "\n",
    "# Inventory Turnover Statistics  \n",
    "print(f\"\\nInventory Turnover Rate:\")\n",
    "print(f\"  Mean turnover: {sku_data['inventory_turnover'].mean():.2f}x\")\n",
    "print(f\"  Median turnover: {sku_data['inventory_turnover'].median():.2f}x\")\n",
    "print(f\"  25th percentile: {sku_data['inventory_turnover'].quantile(0.25):.2f}x\")\n",
    "\n",
    "# Identify slow-moving inventory based on DSI\n",
    "slow_moving_threshold = 100  # SKUs that will take >90 days to sell at current pace\n",
    "slow_moving_skus = sku_data[\n",
    "    (sku_data['days_sales_in_inventory'] > slow_moving_threshold) & \n",
    "    (sku_data['current_inventory'] > 0)\n",
    "]\n",
    "\n",
    "print(f\"\\nSlow-Moving Inventory Alert:\")\n",
    "print(f\"  SKUs taking >{slow_moving_threshold} days to sell: {len(slow_moving_skus)}\")\n",
    "print(f\"  Total units in slow-moving inventory: {slow_moving_skus['current_inventory'].sum()}\")\n",
    "\n",
    "# Show top 10 slowest moving SKUs\n",
    "if len(slow_moving_skus) > 0:\n",
    "    print(f\"\\nTop 10 Slowest Moving SKUs:\")\n",
    "    slowest = slow_moving_skus.nlargest(10, 'days_sales_in_inventory')\n",
    "    for idx, row in slowest.iterrows():\n",
    "        print(f\"  {row['sku']}: {row['days_sales_in_inventory']:.0f} days to sell {row['current_inventory']} units\")\n",
    "\n",
    "# Display the updated dataframe with new metrics\n",
    "print(f\"\\nDataframe now includes new columns:\")\n",
    "print(\"  - days_sales_in_inventory: Days to sell current inventory at recent velocity\")\n",
    "print(\"  - inventory_turnover: How many times inventory turns over (higher = better)\")\n",
    "\n",
    "# Show sample of the new columns\n",
    "print(f\"\\nSample of new metrics:\")\n",
    "sample_cols = ['sku', 'name', 'current_inventory', 'velocity_last_30d', \n",
    "               'days_sales_in_inventory', 'inventory_turnover']\n",
    "print(sku_data[sample_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if unit_price is always equal to max_unit_price\n",
    "price_comparison = sku_data[['unit_price', 'max_unit_price']].copy()\n",
    "price_comparison['prices_equal'] = price_comparison['unit_price'] == price_comparison['max_unit_price']\n",
    "\n",
    "print(\"Unit Price vs Max Unit Price Analysis:\")\n",
    "print(f\"Total rows: {len(price_comparison)}\")\n",
    "print(f\"Rows where unit_price == max_unit_price: {price_comparison['prices_equal'].sum()}\")\n",
    "print(f\"Rows where unit_price != max_unit_price: {(~price_comparison['prices_equal']).sum()}\")\n",
    "print(f\"Rows with NaN unit_price: {price_comparison['unit_price'].isna().sum()}\")\n",
    "print(f\"Rows with NaN max_unit_price: {price_comparison['max_unit_price'].isna().sum()}\")\n",
    "\n",
    "# Show examples where they differ\n",
    "different_prices = price_comparison[~price_comparison['prices_equal'] & price_comparison['unit_price'].notna() & price_comparison['max_unit_price'].notna()]\n",
    "if len(different_prices) > 0:\n",
    "    print(f\"\\nExamples where prices differ:\")\n",
    "    print(different_prices.head(10))\n",
    "\n",
    "# Show all rows where max_unit_price is greater than unit_price\n",
    "max_greater_than_unit = sku_data[\n",
    "    (sku_data['max_unit_price'] > sku_data['unit_price']) & \n",
    "    sku_data['unit_price'].notna() & \n",
    "    sku_data['max_unit_price'].notna()\n",
    "]\n",
    "\n",
    "\n",
    "# Fill nan unit_price values with max_unit_price\n",
    "sku_data['unit_price'] = sku_data['unit_price'].fillna(sku_data['max_unit_price'])\n",
    "max_greater_than_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataset(sku_data)\n",
    "sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sku_data\n",
    "df_sku = sku_data[['sku', 'current_inventory', 'cost_per_unit', 'avg_unit_price', 'velocity']]\n",
    "\n",
    "# Add total_cost column\n",
    "df_sku['total_cost'] = df_sku['current_inventory'] * df_sku['cost_per_unit']\n",
    "\n",
    "# Rename columns\n",
    "df_sku = df_sku.rename(columns={\n",
    "    'sku': 'SKU',\n",
    "    'current_inventory': 'on_hand',\n",
    "    'cost_per_unit': 'cost_per_unit',\n",
    "    'total_cost': 'total_cost',\n",
    "    'avg_unit_price': 'avg_price',\n",
    "    'velocity': 'sales_velocity'\n",
    "})\n",
    "\n",
    "df_sku\n",
    "\n",
    "# Set variables\n",
    "# Define discount progression once at the beginning\n",
    "STARTING_DISCOUNT_PCT = 40  # Starting discount percentage\n",
    "WEEKLY_DISCOUNT_INCREMENT = 5  # Discount increase per week\n",
    "\n",
    "# Vitality LTM Data \n",
    "Payroll = 1405788.35  \n",
    "plExpense = 0\n",
    "Occupancy = 3007483.62\n",
    "Miscellaneous = 517152.08  \n",
    "\n",
    "# Caluclate for 12 weeks \n",
    "Payroll = Payroll/52 \n",
    "plExpense = plExpense/52 \n",
    "Occupancy = Occupancy/52 \n",
    "Miscellaneous = Miscellaneous/52\n",
    "Advertising = 10000\n",
    "Liquidation = 3500\n",
    "print(Payroll)\n",
    "print(plExpense)\n",
    "print(Occupancy)\n",
    "print(Miscellaneous)\n",
    "print(Advertising)\n",
    "print(Liquidation)\n",
    "TotalExpenses = Payroll + plExpense + Occupancy + Miscellaneous + Advertising + Liquidation\n",
    "print(TotalExpenses)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DYNAMIC LIQUIDATION ANALYSIS WITH VELOCITY MULTIPLIER MODEL\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def calculate_sku_velocities_at_discounts(df_sku, max_weeks=20):\n",
    "    \"\"\"\n",
    "    Calculate velocity for each SKU at specific discount levels by week.a\n",
    "    Dynamic weeks starting at 30% discount, adding 5% every week until max_weeks\n",
    "    \n",
    "    Args:\n",
    "        df_sku: DataFrame with SKU data including sales_velocity\n",
    "        max_weeks: Maximum number of weeks to calculate (default 20)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with original columns plus new velocity columns at different discount levels\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define discount levels dynamically using the global constants\n",
    "    discount_levels = [STARTING_DISCOUNT_PCT + (i * WEEKLY_DISCOUNT_INCREMENT) for i in range(max_weeks)]\n",
    "    \n",
    "    # Start with a copy of the original dataframe\n",
    "    result_df = df_sku.copy()\n",
    "    \n",
    "    # For each discount level, calculate the velocity\n",
    "    for discount in discount_levels:\n",
    "        velocities = []\n",
    "        for _, row in df_sku.iterrows():\n",
    "            base_velocity = row['sales_velocity']\n",
    "            \n",
    "            # Calculate velocity multiplier using the Hills equation\n",
    "            # y = 1 + (U-1) * x^n / (C^n + x^n)\n",
    "            # Parameters: U, n, and C\n",
    "            x = discount\n",
    "            U = 5.0  # Maximum velocity multiplier\n",
    "            n = 5.0   # Hill coefficient (cooperativity)\n",
    "            C = 50.0  # Half-maximal discount percentage\n",
    "            \n",
    "            velocity_multiplier = 1 + (U - 1) * (x ** n) / (C ** n + x ** n)\n",
    "                \n",
    "            predicted_velocity = base_velocity * velocity_multiplier\n",
    "            \n",
    "            velocities.append(round(predicted_velocity, 3))\n",
    "        \n",
    "        result_df[f'velocity_at_{discount}%'] = velocities\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "\n",
    "def simulate_dynamic_liquidation(df_enhanced, total_expenses, max_weeks=20):\n",
    "    \"\"\"\n",
    "    Simulate dynamic liquidation period with increasing discounts.\n",
    "    Stops when weekly gross recovery minus dynamic fee drops below total_expenses.\n",
    "    Starting at defined discount percentage, adding increment every week.\n",
    "    \n",
    "    Args:\n",
    "        df_enhanced: DataFrame with SKU data and velocity calculations\n",
    "        total_expenses: Weekly expense threshold to stop liquidation (excluding fee)\n",
    "        max_weeks: Maximum weeks to simulate (safety limit)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (liquidation_df, actual_weeks_simulated, summary_data, final_week_stopped)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create base DataFrame with SKU and initial inventory\n",
    "    liquidation_df = df_enhanced[['SKU', 'on_hand']].copy()\n",
    "    liquidation_df = liquidation_df.rename(columns={'on_hand': 'Week_0_Inventory'})\n",
    "    \n",
    "    # Track remaining inventory for each week\n",
    "    remaining_inventory = df_enhanced['on_hand'].copy()\n",
    "    \n",
    "    # Initialize summary data\n",
    "    summary_data = []\n",
    "    \n",
    "    # Get SKU data for calculations\n",
    "    sku_data = df_enhanced[['SKU', 'on_hand', 'cost_per_unit', 'avg_price']].copy()\n",
    "    \n",
    "    week = 1\n",
    "    continue_liquidation = True\n",
    "    final_week_stopped = None\n",
    "    \n",
    "    print(f\"🚀 Starting dynamic liquidation simulation...\")\n",
    "    print(f\"💰 Weekly expense threshold (excluding fee): ${total_expenses:,.2f}\")\n",
    "    print(f\"⏹️  Liquidation will stop when (gross recovery - 5% fee) < ${total_expenses:,.2f}\")\n",
    "    print()\n",
    "    \n",
    "    while continue_liquidation and week <= max_weeks:\n",
    "        discount_pct = STARTING_DISCOUNT_PCT + ((week - 1) * WEEKLY_DISCOUNT_INCREMENT)\n",
    "        \n",
    "        # Check if we have velocity data for this discount level\n",
    "        velocity_col = f'velocity_at_{discount_pct}%'\n",
    "        if velocity_col not in df_enhanced.columns:\n",
    "            print(f\"⚠️  No velocity data for week {week} (discount {discount_pct}%). Stopping simulation.\")\n",
    "            break\n",
    "        \n",
    "        # Calculate weekly sales\n",
    "        weekly_sales = df_enhanced[velocity_col] * 7  # 7 days per week\n",
    "        \n",
    "        # Calculate units sold this week (before updating inventory)\n",
    "        prev_inventory = remaining_inventory.copy()\n",
    "        remaining_inventory = (remaining_inventory - weekly_sales).clip(lower=0)\n",
    "        units_sold_by_sku = prev_inventory - remaining_inventory\n",
    "        \n",
    "        # Calculate financial metrics for this week\n",
    "        inventory_cost = (units_sold_by_sku * sku_data['cost_per_unit']).sum()\n",
    "        inventory_sp = (units_sold_by_sku * sku_data['avg_price']).sum()\n",
    "        \n",
    "        # Calculate gross recovery (applying discount)\n",
    "        selling_price_discounted = sku_data['avg_price'] * (1 - discount_pct / 100)\n",
    "        gross_recovery = (units_sold_by_sku * selling_price_discounted).sum()\n",
    "        \n",
    "        # Calculate dynamic fee (5% of gross recovery)\n",
    "        dynamic_fee = 0.05 * gross_recovery\n",
    "        net_recovery = gross_recovery - dynamic_fee\n",
    "        \n",
    "        # Store inventory results\n",
    "        liquidation_df[f'Week_{week}_Inventory'] = remaining_inventory.round(0).astype(int)\n",
    "        \n",
    "        # Store summary data\n",
    "        pct_of_cost = (gross_recovery / inventory_cost * 100) if inventory_cost > 0 else 0\n",
    "        pct_of_sp = (gross_recovery / inventory_sp * 100) if inventory_sp > 0 else 0\n",
    "        \n",
    "        # Check if we should continue (net recovery >= total expenses)\n",
    "        if net_recovery < total_expenses:\n",
    "            print(f\"🛑 Stopping liquidation at week {week}:\")\n",
    "            print(f\"   Gross recovery: ${gross_recovery:,.2f}\")\n",
    "            print(f\"   Dynamic fee (5%): ${dynamic_fee:,.2f}\")\n",
    "            print(f\"   Net recovery: ${net_recovery:,.2f}\")\n",
    "            print(f\"   Total expenses: ${total_expenses:,.2f}\")\n",
    "            print(f\"   Discount level: {discount_pct:.1f}%\")\n",
    "            final_week_stopped = week\n",
    "            continue_liquidation = False\n",
    "        else:\n",
    "            # Only add profitable weeks to summary data\n",
    "            summary_data.append({\n",
    "                'Week': week,\n",
    "                'Units_Sold': units_sold_by_sku.sum(),\n",
    "                'Inventory_Cost': inventory_cost,\n",
    "                'Inventory_SP': inventory_sp,\n",
    "                'Discount_Pct': discount_pct,\n",
    "                'Gross_Recovery': gross_recovery,\n",
    "                'Dynamic_Fee': dynamic_fee,\n",
    "                'Net_Recovery': net_recovery,\n",
    "                'Pct_of_Cost': pct_of_cost,\n",
    "                'Pct_of_SP': pct_of_sp\n",
    "            })\n",
    "            print(f\"✅ Week {week}: Net recovery ${net_recovery:,.2f} > ${total_expenses:,.2f} expenses ({discount_pct:.1f}% discount)\")\n",
    "        \n",
    "        week += 1\n",
    "    \n",
    "    actual_weeks = len(summary_data)  # Only count profitable weeks\n",
    "    print(f\"\\n📊 Liquidation simulation completed after {actual_weeks} profitable weeks\")\n",
    "    if final_week_stopped:\n",
    "        print(f\"📈 Week {final_week_stopped} would have been unprofitable and was excluded\")\n",
    "    \n",
    "    return liquidation_df, actual_weeks, summary_data, final_week_stopped\n",
    "\n",
    "\n",
    "# Calculate velocities for all SKUs at extended discount levels\n",
    "print(\"🎯 Calculating dynamic sigmoid velocity model for all SKUs...\")\n",
    "print(f\"📈 Model: velocity = base_velocity × sigmoid_multiplier(discount)\")\n",
    "print(f\"   Sigmoid equation: y = 22.605 * x^5.153 / (78.069^5.153 + x^5.153) + 0.981\")\n",
    "print(f\"   Where x = discount percentage and y = velocity multiplier\")\n",
    "print(f\"📊 Discount progression: starts at {STARTING_DISCOUNT_PCT:.1f}%, increases by {WEEKLY_DISCOUNT_INCREMENT:.1f}% per week\")\n",
    "print()\n",
    "\n",
    "# Check if df_sku is properly defined before proceeding\n",
    "if 'df_sku' not in locals() or df_sku is None:\n",
    "    print(\"❌ ERROR: df_sku is not defined!\")\n",
    "    print(\"🔧 SOLUTION: Run the previous cell first to properly define df_sku\")\n",
    "    raise NameError(\"df_sku is not defined. Please run the previous cell first.\")\n",
    "\n",
    "# Calculate velocities for up to 20 weeks (safety buffer)\n",
    "df_sku_enhanced = calculate_sku_velocities_at_discounts(df_sku, max_weeks=20)\n",
    "\n",
    "print(f\"✅ Calculated velocities for {len(df_sku_enhanced)} SKUs for up to 20 weeks\")\n",
    "print()\n",
    "\n",
    "# Run the dynamic liquidation simulation\n",
    "df_liquidation, actual_weeks, summary_data, final_week_stopped = simulate_dynamic_liquidation(\n",
    "    df_sku_enhanced, \n",
    "    TotalExpenses,  # Use the calculated weekly expense threshold (excluding fee)\n",
    "    max_weeks=20\n",
    ")\n",
    "\n",
    "print(f\"\\n🔍 Sample results (first 5 SKUs) for {actual_weeks} weeks:\")\n",
    "# Show sample columns dynamically based on actual weeks simulated\n",
    "sample_cols = ['SKU', 'Week_0_Inventory', 'Week_1_Inventory']\n",
    "if actual_weeks >= 2:\n",
    "    sample_cols.append('Week_2_Inventory')\n",
    "if actual_weeks >= 5:\n",
    "    sample_cols.append('Week_5_Inventory')\n",
    "if actual_weeks >= 8:\n",
    "    sample_cols.append('Week_8_Inventory')\n",
    "sample_cols.append(f'Week_{actual_weeks}_Inventory')\n",
    "\n",
    "print(df_liquidation[sample_cols].head())\n",
    "\n",
    "\n",
    "# Calculate total starting inventory cost for percentage calculations\n",
    "sku_data = df_sku_enhanced[['SKU', 'on_hand', 'cost_per_unit', 'avg_price']].copy()\n",
    "total_starting_cost = (df_liquidation['Week_0_Inventory'] * sku_data['cost_per_unit']).sum()\n",
    "\n",
    "# Create and display the formatted table (only profitable weeks)\n",
    "print(f\"\\n📊 DYNAMIC LIQUIDATION ANALYSIS TABLE (Profitable Weeks Only)\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"{'Week':<6} {'Inventory':<15} {'% of':<8} {'Inventory at':<15} {'Gross':<15} {'% of':<8} {'% of':<8}\")\n",
    "print(f\"{'':6} {'Cost $':<15} {'Liquidated':<8} {'SP $':<15} {'Recovery $':<15} {'Cost':<8} {'SP':<8}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "# Calculate totals (only from profitable weeks)\n",
    "total_inv_cost = sum(data['Inventory_Cost'] for data in summary_data)\n",
    "total_inv_sp = sum(data['Inventory_SP'] for data in summary_data)\n",
    "total_gross_recovery = sum(data['Gross_Recovery'] for data in summary_data)\n",
    "total_dynamic_fee = sum(data['Dynamic_Fee'] for data in summary_data)\n",
    "total_net_recovery = sum(data['Net_Recovery'] for data in summary_data)\n",
    "total_pct_cost = (total_gross_recovery / total_inv_cost * 100) if total_inv_cost > 0 else 0\n",
    "total_pct_sp = (total_gross_recovery / total_inv_sp * 100) if total_inv_sp > 0 else 0\n",
    "total_pct_of_liquidated = (total_inv_cost / total_inv_cost * 100) if total_inv_cost > 0 else 0  # This will be 100%\n",
    "\n",
    "for data in summary_data:\n",
    "    week = data['Week']\n",
    "    inv_cost = data['Inventory_Cost']\n",
    "    pct_liquidated = (data['Inventory_Cost'] / total_inv_cost * 100) if total_inv_cost > 0 else 0  # % of liquidated inventory, not total inventory\n",
    "    inv_sp = data['Inventory_SP']\n",
    "    gross_rec = data['Gross_Recovery']\n",
    "    pct_cost = data['Pct_of_Cost']\n",
    "    pct_sp = data['Pct_of_SP']\n",
    "    \n",
    "    print(f\"{week:<6} ${inv_cost:<14,.0f} {pct_liquidated:<7.1f}% ${inv_sp:<14,.0f} ${gross_rec:<14,.0f} {pct_cost:<7.1f}% {pct_sp:<7.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'TOTAL':<6} ${total_inv_cost:<14,.0f} {total_pct_of_liquidated:<7.1f}% ${total_inv_sp:<14,.0f} ${total_gross_recovery:<14,.0f} {total_pct_cost:<7.1f}% {total_pct_sp:<7.1f}%\")\n",
    "\n",
    "print()\n",
    "print(\"💡 Key Insights:\")\n",
    "print(f\"   • Starting inventory value: ${total_starting_cost:,.0f} at cost\")\n",
    "print(f\"   • Inventory liquidated in profitable weeks: ${total_inv_cost:,.0f} ({(total_inv_cost/total_starting_cost*100) if total_starting_cost>0 else 0:.1f}% of total)\")\n",
    "print(f\"   • Expected total gross recovery: ${total_gross_recovery:,.0f} ({total_pct_cost:.1f}% of liquidated cost)\")\n",
    "print(f\"   • Total dynamic fees (5%): ${total_dynamic_fee:,.0f}\")\n",
    "print(f\"   • Expected total net recovery: ${total_net_recovery:,.0f} ({(total_net_recovery/total_inv_cost*100) if total_inv_cost>0 else 0:.1f}% of liquidated cost)\")\n",
    "print(f\"   • Profitable liquidation weeks: {actual_weeks}\")\n",
    "if final_week_stopped:\n",
    "    print(f\"   • Week {final_week_stopped} would have been unprofitable and was excluded\")\n",
    "discount_progression = [f\"{STARTING_DISCOUNT_PCT + ((i-1) * WEEKLY_DISCOUNT_INCREMENT):.0f}%\" for i in range(1, actual_weeks + 1)]\n",
    "print(f\"   • Discount progression: {', '.join(discount_progression)}\")\n",
    "print(f\"   • Weekly expense threshold (excluding fee): ${TotalExpenses:,.2f}\")\n",
    "\n",
    "# Show inventory aging analysis\n",
    "print()\n",
    "print(\"📈 DYNAMIC INVENTORY AGING BREAKDOWN:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate what percentage of inventory sells out in each time period\n",
    "total_starting_units = df_liquidation['Week_0_Inventory'].sum()\n",
    "\n",
    "print(f\"{'Age Bucket':<15} {'Units':<10} {'% of Total':<12} {'Cumulative %':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "cumulative_sold = 0\n",
    "\n",
    "# Week 1\n",
    "if actual_weeks >= 1:\n",
    "    units_sold = df_liquidation['Week_0_Inventory'].sum() - df_liquidation['Week_1_Inventory'].sum()\n",
    "    cumulative_sold += units_sold\n",
    "    pct_total = (units_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "    cumulative_pct = (cumulative_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "    print(f\"{'Week 1':<15} {units_sold:<10,} {pct_total:<11.1f}% {cumulative_pct:<14.1f}%\")\n",
    "\n",
    "# Week 2\n",
    "if actual_weeks >= 2:\n",
    "    units_sold = df_liquidation['Week_1_Inventory'].sum() - df_liquidation['Week_2_Inventory'].sum()\n",
    "    cumulative_sold += units_sold\n",
    "    pct_total = (units_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "    cumulative_pct = (cumulative_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "    print(f\"{'Week 2':<15} {units_sold:<10,} {pct_total:<11.1f}% {cumulative_pct:<14.1f}%\")\n",
    "\n",
    "# Weeks 3 to middle point\n",
    "if actual_weeks >= 3:\n",
    "    mid_week = min(6, actual_weeks)\n",
    "    if mid_week > 2:\n",
    "        start_inventory = df_liquidation['Week_2_Inventory'].sum()\n",
    "        end_inventory = df_liquidation[f'Week_{mid_week}_Inventory'].sum()\n",
    "        units_sold = start_inventory - end_inventory\n",
    "        cumulative_sold += units_sold\n",
    "        pct_total = (units_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "        cumulative_pct = (cumulative_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "        print(f\"{'Weeks 3-' + str(mid_week):<15} {units_sold:<10,} {pct_total:<11.1f}% {cumulative_pct:<14.1f}%\")\n",
    "\n",
    "# Remaining weeks\n",
    "if actual_weeks > 6:\n",
    "    start_inventory = df_liquidation['Week_6_Inventory'].sum()\n",
    "    end_inventory = df_liquidation[f'Week_{actual_weeks}_Inventory'].sum()\n",
    "    units_sold = start_inventory - end_inventory\n",
    "    cumulative_sold += units_sold\n",
    "    pct_total = (units_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "    cumulative_pct = (cumulative_sold / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "    print(f\"{f'Weeks 7-{actual_weeks}':<15} {units_sold:<10,} {pct_total:<11.1f}% {cumulative_pct:<14.1f}%\")\n",
    "\n",
    "# Remaining inventory\n",
    "remaining = df_liquidation[f'Week_{actual_weeks}_Inventory'].sum()\n",
    "remaining_pct = (remaining / total_starting_units * 100) if total_starting_units > 0 else 0\n",
    "print(f\"{'Remaining':<15} {remaining:<10,} {remaining_pct:<11.1f}% {'100.0%':<14}\")\n",
    "\n",
    "print()\n",
    "print(f\"🎯 DYNAMIC LIQUIDATION SUMMARY:\")\n",
    "print(f\"   • Profitable liquidation weeks: {actual_weeks}\")\n",
    "if final_week_stopped:\n",
    "    print(f\"   • Simulation stopped at week {final_week_stopped} (would have been unprofitable)\")\n",
    "print(f\"   • Total units liquidated in profitable weeks: {total_starting_units - remaining:,} ({100 - remaining_pct:.1f}%)\")\n",
    "print(f\"   • SKUs completely sold out: {(df_liquidation[f'Week_{actual_weeks}_Inventory'] == 0).sum()} out of {len(df_liquidation)}\")\n",
    "print(f\"   • Weekly expense threshold (excluding fee): ${TotalExpenses:,.2f}\")\n",
    "if summary_data:\n",
    "    print(f\"   • Final profitable week gross recovery: ${summary_data[-1]['Gross_Recovery']:,.2f}\")\n",
    "    print(f\"   • Final profitable week dynamic fee (5%): ${summary_data[-1]['Dynamic_Fee']:,.2f}\")\n",
    "    print(f\"   • Final profitable week net recovery: ${summary_data[-1]['Net_Recovery']:,.2f}\")\n",
    "print(f\"   • Liquidation efficiency: Net recovery covered expenses for {actual_weeks} weeks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Plotly visualizations for the liquidation analysis\n",
    "\n",
    "# Use the summary_data from the main liquidation analysis table above\n",
    "# summary_data is a list of dicts, one per week, with keys:\n",
    "# 'Week', 'Units_Sold', 'Inventory_Cost', 'Inventory_SP', 'Discount_Pct', 'Gross_Recovery', 'Pct_of_Cost', 'Pct_of_SP'\n",
    "\n",
    "# Extract data for plotting\n",
    "weeks = [d['Week'] for d in summary_data]\n",
    "inv_costs = [d['Inventory_Cost'] for d in summary_data]\n",
    "gross_recoveries = [d['Gross_Recovery'] for d in summary_data]\n",
    "pct_of_costs = [d['Pct_of_Cost'] for d in summary_data]\n",
    "pct_of_sps = [d['Pct_of_SP'] for d in summary_data]\n",
    "inv_sps = [d['Inventory_SP'] for d in summary_data]\n",
    "\n",
    "# 1. Graph: % of Total Inventory Cost Liquidated Each Week (as in the table: Inventory_Cost / total_starting_cost)\n",
    "total_starting_cost = (df_liquidation['Week_0_Inventory'] * \n",
    "                       df_liquidation.merge(df_sku_enhanced[['SKU', 'cost_per_unit']], on='SKU')['cost_per_unit']).sum()\n",
    "pct_of_total_cost = [(cost / total_starting_cost * 100) if total_starting_cost > 0 else 0 for cost in inv_costs]\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=weeks,\n",
    "    y=pct_of_total_cost,\n",
    "    mode='lines+markers',\n",
    "    name='% of Total Inventory Cost Liquidated',\n",
    "    line=dict(color='rgb(55, 83, 109)', width=3),\n",
    "    marker=dict(size=8, color='rgb(55, 83, 109)'),\n",
    "    text=[f'{pct:.1f}%' for pct in pct_of_total_cost],\n",
    "    textposition='top center'\n",
    "))\n",
    "fig1.update_layout(\n",
    "    title=f'Dynamic Liquidation Analysis - % of Total Inventory Cost Liquidated ({actual_weeks} Weeks)',\n",
    "    xaxis_title='Week',\n",
    "    yaxis_title='% of Total Inventory Cost',\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    "    width=900,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig1.update_xaxes(tickmode='linear', tick0=1, dtick=1)\n",
    "fig1.show()\n",
    "\n",
    "# 2. Graph: Recovery Rate as % of Original Selling Price (Pct_of_SP from summary_data)\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x=weeks,\n",
    "    y=pct_of_sps,\n",
    "    mode='lines+markers',\n",
    "    name='% of SP Recovery',\n",
    "    line=dict(color='rgb(255, 65, 54)', width=3),\n",
    "    marker=dict(size=8, color='rgb(255, 65, 54)'),\n",
    "    text=[f'{pct:.1f}%' for pct in pct_of_sps],\n",
    "    textposition='top center'\n",
    "))\n",
    "fig2.update_layout(\n",
    "    title=f'Dynamic Recovery Analysis - % of Original Selling Price ({actual_weeks} Weeks)',\n",
    "    xaxis_title='Week',\n",
    "    yaxis_title='% of Original Selling Price',\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    "    width=900,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig2.update_xaxes(tickmode='linear', tick0=1, dtick=1)\n",
    "fig2.show()\n",
    "\n",
    "# 3. Graph: Recovery Rate as % of Original Cost (Pct_of_Cost from summary_data)\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(go.Scatter(\n",
    "    x=weeks,\n",
    "    y=pct_of_costs,\n",
    "    mode='lines+markers',\n",
    "    name='% of Cost Recovery',\n",
    "    line=dict(color='rgb(50, 171, 96)', width=3),\n",
    "    marker=dict(size=8, color='rgb(50, 171, 96)'),\n",
    "    text=[f'{pct:.1f}%' for pct in pct_of_costs],\n",
    "    textposition='top center'\n",
    "))\n",
    "fig3.update_layout(\n",
    "    title=f'Dynamic Recovery Analysis - % of Original Cost ({actual_weeks} Weeks)',\n",
    "    xaxis_title='Week',\n",
    "    yaxis_title='% of Original Cost',\n",
    "    showlegend=False,\n",
    "    height=500,\n",
    "    width=900,\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig3.update_xaxes(tickmode='linear', tick0=1, dtick=1)\n",
    "fig3.show()\n",
    "\n",
    "print(\"📊 Dynamic Liquidation Analysis Visualizations Complete!\")\n",
    "print(\"   • Graph 1: Weekly percentage of total inventory cost liquidated\")\n",
    "print(\"   • Graph 2: Recovery rate as percentage of original selling price\")\n",
    "print(\"   • Graph 3: Recovery rate as percentage of original cost\")\n",
    "print(f\"   • Analysis covers {actual_weeks} weeks until recovery dropped below ${TotalExpenses:,.2f} expense threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this to sample rows for testing\n",
    "#sku_data = sku_data.sample(n=50, random_state=None)\n",
    "#sku_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 rows of sku_data\n",
    "#print(\"First 10 rows of sku_data:\")\n",
    "#print(sku_data.head(10).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_data = df\n",
    "\n",
    "sku_data\n",
    "inspect_dataset(sku_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataset(sku_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4.1\"  \n",
    "\n",
    "# 1) DISCOVERY  ─ returns {\"attr\": [examples,…], …}\n",
    "# ─────────────────────────────────────────────────────────\n",
    "DISCOVER_SCHEMA = {\n",
    "    \"name\": \"list_attributes\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\"attributes\"],\n",
    "        \"properties\": {\n",
    "            \"attributes\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": [\"string\", \"number\", \"boolean\", \"null\"]}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "DISCOVER_SYSTEM = \"\"\"\n",
    "You are a generic attribute‐discovery AI.  \n",
    "Given a list of product names, return a JSON mapping:\n",
    "  \"attributes\": { \"<attr_name>\": [<example_values…>] }  \n",
    "— use snake_case keys for booleans (is_ or has_) and pascal_case for everything else.  \n",
    "— values may be strings, numbers, or booleans. booleans are preferred when possible. For boolean attributes, have them start with is_ or has_ and be true or false. \n",
    "— don’t invent attributes, only those present or logically implied. Don't be over specific, be general.\n",
    "- do not repeat the same attributes\n",
    "- do not incude none as an attribute\n",
    "- for string attributes, dont make the attributes overlap with each other, so an attribute  shouldnt have multiplte strings apply to it. \n",
    "- for outlier products, add product_type as a string attribute for those (be sure to incude the main product itself in this list), and feel free to use null for those for the other string and number (very rarly boolean)attributes that are not applicable.\n",
    "- attributes shouldnt generally be more than 2 words\n",
    "- attributes should be mutually exclusive\n",
    "Return only the function call:\n",
    "```python\n",
    "list_attributes(attributes={...})\n",
    "\"\"\"\n",
    "\n",
    "def discover_attributes(names: pd.Series, retries=2) -> dict:\n",
    "    bullets = \"\\n\".join(f\"- {n}\" for n in names.tolist())\n",
    "    user_prompt = f\"Sample SKU names:\\n{bullets}\\n\"\n",
    "\n",
    "    for _ in range(retries):\n",
    "        r = openai.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            temperature=0,\n",
    "            tools=[{\"type\": \"function\", \"function\": DISCOVER_SCHEMA}],\n",
    "            tool_choice={\"type\": \"function\",\n",
    "                         \"function\": {\"name\": \"list_attributes\"}},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": DISCOVER_SYSTEM},\n",
    "                {\"role\": \"user\",   \"content\": user_prompt},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        payload = json.loads(\n",
    "            r.choices[0].message.tool_calls[0].function.arguments\n",
    "        )\n",
    "        attrs = payload[\"attributes\"]\n",
    "        if attrs:                      # got a non-empty dict\n",
    "            return attrs\n",
    "\n",
    "    # if we fall through the loop, raise instead of returning None\n",
    "    raise RuntimeError(\"Attribute discovery failed after retries\")\n",
    "\n",
    "DISCOVERED = discover_attributes(sku_data[\"name\"])   # save this\n",
    "# right after discovery, strip duplicates but keep order:\n",
    "for k, vals in DISCOVERED.items():\n",
    "    DISCOVERED[k] = list(dict.fromkeys(vals))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def format_discovered_attributes(discovered: dict):\n",
    "    type_groups = defaultdict(list)\n",
    "\n",
    "    for attr, values in discovered.items():\n",
    "        # Check the type of the first non-null value\n",
    "        sample_type = type(next((v for v in values if v is not None), None))\n",
    "\n",
    "        if sample_type is bool:\n",
    "            type_groups['Booleans'].append((attr, values))\n",
    "        elif sample_type in [int, float]:\n",
    "            type_groups['Numbers'].append((attr, values))\n",
    "        else:\n",
    "            type_groups['Strings'].append((attr, values))\n",
    "\n",
    "    for category in ['Booleans', 'Numbers', 'Strings']:\n",
    "        print(f\"\\n## {category}\\n\")\n",
    "        for attr, values in sorted(type_groups[category]):\n",
    "            print(f\"{attr}: {values}\")\n",
    "\n",
    "# Call the formatting function\n",
    "format_discovered_attributes(DISCOVERED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────\n",
    "# 2)  EXTRACTION  (patched)\n",
    "# ─────────────────────────────────────────────────────────\n",
    "def make_extract_schema(attr_keys: list[str], discovered: dict) -> dict:\n",
    "    props = {\n",
    "        k: {\"enum\": list(discovered.get(k, [])) + [None]}\n",
    "        for k in attr_keys\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"name\": \"extract_attributes\",\n",
    "        \"description\": \"Return attributes for a batch of SKUs in order.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"items\": {                     # top-level list of SKU objects\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": props,\n",
    "                        \"required\": attr_keys,\n",
    "                        \"additionalProperties\": False,\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"items\"],\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "EXTRACT_SYSTEM = \"\"\"\n",
    "You are an attribute-extraction AI.\n",
    "\n",
    "Return exactly one call to `extract_attributes` whose arguments are:\n",
    "\n",
    "{\n",
    "  \"items\": [\n",
    "     {\"color\": \"Black\", \"is_readers\": true, ...},\n",
    "     ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "Strings / numbers / booleans only; unknown → null.\n",
    "For boolean attributes (is_/has_): true if implied, false if absent.\n",
    "For string and number attributes, null is very rare.\n",
    "Boolean attributes should be true if implied, false if absent, never null.\n",
    "There might be some error/mistakes in the data, so use your best judgement.\n",
    "\"\"\"\n",
    "\n",
    "def extract_batch(names: list[str], attr_keys: list[str], schema) -> list[dict]:\n",
    "    bullets = \"\\n\".join(f\"- {n}\" for n in names)\n",
    "    user_msg = (\n",
    "        \"For each SKU below, return an *array* of objects with these keys:\\n\"\n",
    "        f\"{', '.join(attr_keys)}\\n\"\n",
    "        \"One object per SKU, same order:\\n\"\n",
    "        + bullets\n",
    "    )\n",
    "\n",
    "    r = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        temperature=0,\n",
    "        tools=[{\"type\": \"function\", \"function\": schema}],     # ← wrap!\n",
    "        tool_choice={\"type\": \"function\",\n",
    "                     \"function\": {\"name\": \"extract_attributes\"}},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EXTRACT_SYSTEM},\n",
    "            {\"role\": \"user\",   \"content\": user_msg},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Tool call payload → JSON\n",
    "    args_json = r.choices[0].message.tool_calls[0].function.arguments\n",
    "    raw_list = json.loads(args_json)[\"items\"]\n",
    "\n",
    "    # pad / reorder\n",
    "    return [{k: d.get(k, None) for k in attr_keys} for d in raw_list]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────\n",
    "# 3) PIPELINE  ─ enrich df + one‑hot selected attrs\n",
    "# ─────────────────────────────────────────────────────────\n",
    "def enrich_df(\n",
    "    df: pd.DataFrame,\n",
    "    name_col: str = \"name\",\n",
    "    force_attrs: tuple[str, ...] = (),\n",
    "    one_hot: tuple[str, ...] = (),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    • discovers attributes,\n",
    "    • extracts them for every SKU,\n",
    "    • one‑hots selected attributes,\n",
    "    • returns df with new columns.\n",
    "    \"\"\"\n",
    "    discovered = DISCOVERED\n",
    "    print(\"Discovered:\", discovered.keys())\n",
    "    # merge discovered + forced list (keeps order, no dups)\n",
    "    attr_keys = list(dict.fromkeys(list(discovered.keys()) + list(force_attrs)))\n",
    "    schema = make_extract_schema(attr_keys, discovered)\n",
    "\n",
    "        # ----- run GPT in 10-SKU batches (≈10× faster) -----\n",
    "    BATCH = 10\n",
    "    rows = []\n",
    "  \n",
    "    for i in tqdm(range(0, len(df), BATCH), desc=\"Extracting batches\"):\n",
    "        chunk = df[name_col].iloc[i : i + BATCH].tolist()\n",
    "        batch_results = extract_batch(chunk, attr_keys, schema)\n",
    "        rows.extend(batch_results)\n",
    "        \n",
    "    \n",
    "    feat = pd.DataFrame(rows, columns=attr_keys)  # ensures all cols exist\n",
    "    \n",
    "    feat = feat.where(pd.notnull(feat), pd.NA)\n",
    "    feat = (pd.DataFrame(rows, columns=attr_keys)\n",
    "          .astype(object)              # keeps Python None\n",
    "          .where(pd.notnull, None))    # convert any stray NaN → None\n",
    "\n",
    "\n",
    "    # ----- one‑hot -----\n",
    "    for col in one_hot:\n",
    "        if col not in feat:          # skip if discovery missed it\n",
    "            continue\n",
    "        dummies = pd.get_dummies(feat[col]).astype(bool)\n",
    "        dummies.columns = [f\"{col}__{c}\" for c in dummies.columns]\n",
    "        feat = pd.concat([feat.drop(columns=[col]), dummies], axis=1)\n",
    "    \n",
    "    # After one-hot encoding\n",
    "    feat = feat.loc[:, feat.any()]\n",
    "\n",
    "    # ----- return enriched -----\n",
    "    return pd.concat([df.reset_index(drop=True), feat], axis=1)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────\n",
    "# 4) RUN\n",
    "# ─────────────────────────────────────────────────────────\n",
    "sku_enriched = enrich_df(sku_data)\n",
    "\n",
    "# peek\n",
    "print(sku_enriched.columns[len(sku_data.columns):])  # Print only the new columns\n",
    "print(sku_enriched.columns[len(sku_data.columns):].to_list())  # Print the names of the new columns\n",
    "sku_enriched[[\"name\"] + list(sku_enriched.columns[len(sku_data.columns):])]  # Print the first few rows of the new columns along with the name column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting new columns (excluding original sku_data columns):\")\n",
    "new_columns = sku_enriched.columns[len(sku_data.columns):].to_list()\n",
    "print(\"New columns:\", new_columns)\n",
    "\n",
    "print(\"\\nPreview of 'name' column along with the new columns:\")\n",
    "print(sku_enriched[[\"name\"] + new_columns].head())\n",
    "\n",
    "# Calculate and print the number of null values for each column\n",
    "print(\"\\nNull counts per column:\")\n",
    "subset = sku_enriched[[\"name\"] + new_columns]\n",
    "null_counts = subset.isnull().sum()\n",
    "for col, count in null_counts.items():\n",
    "    print(f\"• {col}: {count} null(s)\")\n",
    "\n",
    "# For each column, print the names of the SKUs where null values occur\n",
    "print(\"\\nSKUs with null values for each column:\")\n",
    "for col in subset.columns:\n",
    "    null_skus = subset[subset[col].isnull()][\"name\"].tolist()\n",
    "    print(f\"• {col}: {null_skus}\")\n",
    "\n",
    "# Special check for the 'name' column (it should not have any nulls)\n",
    "if sku_enriched[\"name\"].isnull().any():\n",
    "    print(\"\\nWARNING: 'name' column contains null values! Please check the above SKUs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_enriched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify attribute columns \n",
    "# “base” = every column that already existed in the raw sku_data table\n",
    "base_cols   = set(sku_data.columns)\n",
    "attr_cols   = [c for c in sku_enriched.columns if c not in base_cols]\n",
    "\n",
    "# classify by type\n",
    "def _is_bool(col):\n",
    "    s = sku_enriched[col].dropna().unique()\n",
    "    return (len(s) <= 2) and set(s).issubset({True, False})\n",
    "\n",
    "bool_cols = [c for c in attr_cols if _is_bool(c)]\n",
    "\n",
    "# treat anything with a *manageable* number of distinct values as a category.\n",
    "cat_cols  = [c for c in attr_cols\n",
    "             if c not in bool_cols]\n",
    "\n",
    "# everything else is ignored for bar‑plots but can still be used later if needed\n",
    "print(\"AI generated columns:\", attr_cols)\n",
    "print(\"Boolean flags       :\", bool_cols)\n",
    "print(\"Categorical attrs   :\", cat_cols)\n",
    "print(\"Base collumns:\", base_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(df, group_col):\n",
    "    summary = (df.groupby(group_col, dropna=False)\n",
    "               .agg(sku_count=(\"sku\", \"nunique\"),\n",
    "                    total_sales=(\"total_sales\", \"sum\")))\n",
    "    # Add a helper column to flag rows where the group key is null\n",
    "    summary[\"_null\"] = summary.index.to_series().apply(lambda x: pd.isna(x))\n",
    "    # Sort by: non-null groups first (i.e. _null=False) and then by ttotal_sale descending\n",
    "    summary = summary.sort_values(by=[\"_null\", \"total_sales\"], ascending=[True, False])\n",
    "    return summary.drop(columns=[\"_null\"])\n",
    "\n",
    "def create_interactive_chart(df, group_col, title):\n",
    "    \"\"\"Create an interactive Plotly chart for the summary data\"\"\"\n",
    "    summary = make_summary(df, group_col)\n",
    "    \n",
    "    # Convert index to string and handle NaN values\n",
    "    labels = [str(x) if pd.notna(x) else 'Unknown/Missing' for x in summary.index]\n",
    "    \n",
    "    # Create subplot with secondary y-axis\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=('SKU Count', 'Total Sales'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    # Add bar charts\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=labels,\n",
    "            y=summary['sku_count'],\n",
    "            name='SKU Count',\n",
    "            marker_color='lightblue',\n",
    "            text=summary['sku_count'],\n",
    "            textposition='auto',\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=labels,\n",
    "            y=summary['total_sales'],\n",
    "            name='Total Sales',\n",
    "            marker_color='lightcoral',\n",
    "            text=summary['total_sales'],\n",
    "            textposition='auto',\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title_text=f\"{title} - Distribution Analysis\",\n",
    "        title_x=0.5,\n",
    "        showlegend=False,\n",
    "        height=400,\n",
    "        margin=dict(l=50, r=50, t=80, b=50)\n",
    "    )\n",
    "    \n",
    "    # Update x-axis labels\n",
    "    fig.update_xaxes(title_text=\"Category\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Category\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Number of SKUs\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Total Sales\", row=1, col=2)\n",
    "    \n",
    "    return fig, summary\n",
    "\n",
    "# Process boolean columns with interactive charts\n",
    "for col in bool_cols:\n",
    "    print(f\"\\n── {col.upper()} ─────────────────────────────────────\")\n",
    "    fig, summary = create_interactive_chart(sku_enriched, col, col.upper())\n",
    "    fig.show()\n",
    "    print(\"\\nSummary Table:\")\n",
    "    display(summary)\n",
    "\n",
    "# Process categorical columns with interactive charts\n",
    "for col in cat_cols:\n",
    "    print(f\"\\n── {col.upper()} ─────────────────────────────────────\")\n",
    "    fig, summary = create_interactive_chart(sku_enriched, col, col.upper())\n",
    "    fig.show()\n",
    "    print(\"\\nSummary Table:\")\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.colors as pcolors\n",
    "from colorsys import hsv_to_rgb\n",
    "# ----------------- HELPER FUNCTIONS (REFACTORED FOR PLOTLY) ---------------------\n",
    "def make_numeric_palette(levels, cmap_name=\"Viridis\"):\n",
    "    \"\"\"\n",
    "    Map numeric categories onto a continuous Plotly colour map.\n",
    "    \"\"\"\n",
    "    nums = pd.to_numeric(levels, errors=\"coerce\")\n",
    "    if nums.isna().all():\n",
    "        return [\"#CCCCCC\"] * len(levels)\n",
    "    \n",
    "    # Get a Plotly continuous colorscale\n",
    "    cmap = getattr(pcolors.sequential, cmap_name)\n",
    "    \n",
    "    # Normalize the numeric values to map to the colorscale\n",
    "    min_val, max_val = nums.min(), nums.max()\n",
    "    if min_val == max_val: # Avoid division by zero if all values are the same\n",
    "        norm_vals = [0.5] * len(nums)\n",
    "    else:\n",
    "        norm = (nums - min_val) / (max_val - min_val)\n",
    "        norm_vals = norm.fillna(0.5) # Default to middle color for NaNs\n",
    "    \n",
    "    # Map normalized values to colors\n",
    "    return [pcolors.sample_colorscale(cmap, val)[0] if pd.notna(val) else \"#CCCCCC\" for val in norm_vals]\n",
    "\n",
    "def default_palette(n):\n",
    "    \"\"\"\n",
    "    Use a distinct Plotly color palette for better readability.\n",
    "    \"\"\"\n",
    "    if n <= 10:\n",
    "        return pcolors.qualitative.T10[:n]\n",
    "    else:\n",
    "        # Use a longer palette and cycle it if needed\n",
    "        extended = pcolors.qualitative.Vivid + pcolors.qualitative.Light24\n",
    "        return (extended * (n // len(extended) + 1))[:n]\n",
    "\n",
    "import plotly.graph_objects as go # Make sure this import is at the top of your script\n",
    "\n",
    "def get_color(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Checks if a color name is recognized by Plotly.\n",
    "    - If yes, returns the color name.\n",
    "    - If no, generates a unique, deterministic color from the name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Let Plotly's own validator try to recognize the color\n",
    "        go.Figure().add_scatter(marker_color=[name])\n",
    "        return name\n",
    "    except ValueError:\n",
    "        # If Plotly raises an error, the color is unknown.\n",
    "        # Generate a color by hashing the name (your original fallback method).\n",
    "        hue = (abs(hash(name)) % 360) / 360.0\n",
    "        rgb_float = hsv_to_rgb(hue, 0.8, 0.9) # Sightly adjusted for good contrast\n",
    "        rgb_int = tuple(int(c * 255) for c in rgb_float)\n",
    "        return f'#%02x%02x%02x' % rgb_int\n",
    "\n",
    "BOOL_COLOR_MAP = {\n",
    "    True:  \"#2CA02C\",  # green\n",
    "    False: \"#D62728\",  # red\n",
    "    \"True\":  \"#2CA02C\",\n",
    "    \"False\": \"#D62728\",\n",
    "    \"Unknown\": \"#CCCCCC\",\n",
    "    \"nan\": \"#CCCCCC\",\n",
    "    pd.NA: \"#CCCCCC\",\n",
    "    None: \"#CCCCCC\",\n",
    "}\n",
    "\n",
    "def choose_bar_colours(grp_name: str, levels: pd.Series) -> list[str]:\n",
    "    \"\"\"\n",
    "    Decide colouring strategy based on attribute type using Plotly palettes.\n",
    "    \"\"\"\n",
    "    if grp_name in bool_cols:\n",
    "        return levels.map(BOOL_COLOR_MAP).fillna(\"#CCCCCC\").tolist()\n",
    "\n",
    "    if pd.to_numeric(levels, errors=\"coerce\").notna().all():\n",
    "        return make_numeric_palette(levels)\n",
    "\n",
    "    if \"color\" in grp_name.lower():\n",
    "        return [get_color(v) if pd.notna(v) else \"#CCCCCC\" for v in levels]\n",
    "\n",
    "    return default_palette(len(levels))\n",
    "\n",
    "# Enhanced metrics with better descriptions\n",
    "metrics = {\n",
    "    \"Current Stock\": \"current_inventory\",\n",
    "    \"Initial Stock\": \"initial_inventory\",\n",
    "    \"Sales (30d)\": \"sales_last_30d\",\n",
    "    \"Total Sales\": \"total_sales\",\n",
    "    \"Total Revenue\": \"total_revenue\",\n",
    "    \"Revenue (30d)\": \"revenue_last_30d\",\n",
    "    \"Daily Revenue (30d)\": \"avg_daily_revenue_30d\"\n",
    "}\n",
    "\n",
    "# ----------------- DATA PROCESSING (UNCHANGED) -----------------------------\n",
    "def calculate_attribute_variability(df, attr_col, metrics_dict):\n",
    "    \"\"\"Calculate variability by summing absolute differences from mean for each option within an attribute using percentages\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean[attr_col] = df_clean[attr_col].fillna(\"Other\")\n",
    "    if df_clean[attr_col].dtype == 'bool' or df_clean[attr_col].dtype == 'boolean' or any(isinstance(v, (bool, np.bool_)) for v in df_clean[attr_col].dropna().unique()):\n",
    "        df_clean[attr_col] = df_clean[attr_col].astype(str)\n",
    "    grouped = df_clean.groupby(attr_col)[list(metrics_dict.values())].sum()\n",
    "    if grouped.empty or grouped.sum().sum() == 0: return 0\n",
    "    pct_grouped = grouped.div(grouped.sum(axis=0), axis=1).fillna(0) * 100\n",
    "    total_attribute_variability = 0\n",
    "    for option in pct_grouped.index:\n",
    "        option_values = pct_grouped.loc[option].values\n",
    "        option_mean = option_values.mean()\n",
    "        option_variability = sum(abs(value - option_mean) for value in option_values)\n",
    "        total_attribute_variability += option_variability\n",
    "    return total_attribute_variability\n",
    "\n",
    "attributes = [attr for attr in attr_cols if attr in sku_enriched.columns and 2 <= sku_enriched[attr].fillna(\"Other\").nunique()]\n",
    "print(f\"Selected attributes for portfolio analysis: {attributes}\")\n",
    "\n",
    "print(\"Calculating attribute variability...\")\n",
    "attribute_variabilities = {attr: calculate_attribute_variability(sku_enriched, attr, metrics) for attr in attributes}\n",
    "sorted_attributes = sorted(attribute_variabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "attributes = [attr for attr, score in sorted_attributes]\n",
    "print(f\"\\nAttributes ranked by variability (highest first):\")\n",
    "for i, (attr, score) in enumerate(sorted_attributes, 1):\n",
    "    print(f\"  {i}. {attr.replace('_', ' ').title()}: {score:.1f}\")\n",
    "\n",
    "if not attributes:\n",
    "    print(\"No suitable attributes found for portfolio analysis.\")\n",
    "else:\n",
    "    attribute_titles = {attr: attr.replace(\"_\", \" \").title() for attr in attributes}\n",
    "\n",
    "    def format_number(num):\n",
    "        if num >= 1_000_000: return f'{num/1_000_000:.1f}M'\n",
    "        if num >= 1_000: return f'{num/1_000:.1f}K'\n",
    "        return f'{int(num)}'\n",
    "\n",
    "    def get_top5_categories_by_sales(attr, reference_metric=\"total_sales\"):\n",
    "        df_temp = sku_enriched.copy()\n",
    "        df_temp[attr] = df_temp[attr].fillna(\"Other\")\n",
    "        if df_temp[attr].dtype == 'bool' or df_temp[attr].dtype == 'boolean':\n",
    "            df_temp[attr] = df_temp[attr].astype(str)\n",
    "        top_by_sales = df_temp.groupby(attr)[reference_metric].sum().sort_values(ascending=False)\n",
    "        return top_by_sales.head(5).index.tolist()\n",
    "\n",
    "    attribute_color_maps = {}\n",
    "    for attr in attributes:\n",
    "        top5_categories = get_top5_categories_by_sales(attr)\n",
    "        all_categories = top5_categories + [\"Others\"]\n",
    "        colors = choose_bar_colours(attr, pd.Series(all_categories))\n",
    "        attribute_color_maps[attr] = {category: colors[i] if i < len(colors) else '#CCCCCC' for i, category in enumerate(all_categories)}\n",
    "\n",
    "    # ----------------- PLOTLY VISUALIZATION --------------------------------------\n",
    "    fig = make_subplots(\n",
    "        rows=len(attributes),\n",
    "        cols=len(metrics),\n",
    "        subplot_titles=list(metrics.keys()),\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.01,\n",
    "        horizontal_spacing=0.01\n",
    "    )\n",
    "\n",
    "    for r, attr in enumerate(attributes):\n",
    "        top5_categories = get_top5_categories_by_sales(attr)\n",
    "        color_map = attribute_color_maps[attr]\n",
    "        attr_variability = attribute_variabilities[attr]\n",
    "        print(f\"\\nProcessing: {attribute_titles[attr]} - Variability Score: {attr_variability:.1f}\")\n",
    "\n",
    "        for c, (metric_name, metric_col) in enumerate(metrics.items()):\n",
    "            # Aggregate data\n",
    "            df_temp = sku_enriched.copy()\n",
    "            df_temp[attr] = df_temp[attr].fillna(\"Other\")\n",
    "            if df_temp[attr].dtype == 'bool' or df_temp[attr].dtype == 'boolean':\n",
    "                df_temp[attr] = df_temp[attr].astype(str)\n",
    "            \n",
    "            df_full = df_temp.groupby(attr)[metric_col].sum().reset_index().rename(columns={attr: \"level\"})\n",
    "            df_top5 = df_full[df_full[\"level\"].isin(top5_categories)].copy()\n",
    "            df_others = df_full[~df_full[\"level\"].isin(top5_categories)]\n",
    "\n",
    "            if not df_others.empty and df_others[metric_col].sum() > 0:\n",
    "                 df_top5 = pd.concat([df_top5, pd.DataFrame([{\"level\": \"Others\", metric_col: df_others[metric_col].sum()}])], ignore_index=True)\n",
    "\n",
    "            sort_map = {cat: i for i, cat in enumerate(top5_categories + ['Others'])}\n",
    "            df_top5['sort_order'] = df_top5['level'].map(sort_map)\n",
    "            df_top5 = df_top5.sort_values('sort_order').reset_index(drop=True)\n",
    "\n",
    "            total_value = df_top5[metric_col].sum()\n",
    "            df_top5['percentage'] = (df_top5[metric_col] / total_value * 100) if total_value > 0 else 0\n",
    "\n",
    "            # Add bar traces for the stacked bar\n",
    "            for _, row in df_top5.iterrows():\n",
    "                fig.add_trace(go.Bar(\n",
    "                    x=[0], # Single bar at x=0\n",
    "                    y=[row['percentage']],\n",
    "                    name=row['level'],\n",
    "                    marker_color=color_map.get(row['level'], '#CCCCCC'),\n",
    "                    text=f\"{row['percentage']:.1f}%\",\n",
    "                    textposition='inside',\n",
    "                    insidetextanchor='middle',\n",
    "                    hoverinfo='text',\n",
    "                    hovertext=f\"{row['level']}: {format_number(row[metric_col])} ({row['percentage']:.1f}%)\",\n",
    "                    textfont={'color': 'white' if row['percentage'] > 5 else 'rgba(0,0,0,0)', 'size': 10} # Hide small labels\n",
    "                ), row=r+1, col=c+1)\n",
    "\n",
    "            # Add total value annotation at the top\n",
    "            if total_value > 0:\n",
    "                fig.add_annotation(\n",
    "                    x=0, y=103, text=f\"<b>{format_number(total_value)}</b>\",\n",
    "                    showarrow=False, bgcolor=\"lightgray\", opacity=0.8,\n",
    "                    font=dict(size=11), row=r+1, col=c+1\n",
    "                )\n",
    "\n",
    "        # Set Y-axis title for the first column of each row\n",
    "        fig.update_yaxes(title_text=f\"<b>{attribute_titles[attr]}</b>\", row=r+1, col=1)\n",
    "\n",
    "\n",
    "    # ----------------- FIGURE LAYOUT AND STYLING ---------------------------------\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': \"<b>Product Portfolio Analysis: Sales & Inventory Breakdown</b><br><sub>Attributes sorted by variability (highest first)</sub>\",\n",
    "            'y': 0.98,\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top',\n",
    "            'font': {'size': 20}\n",
    "        },\n",
    "        barmode='stack',\n",
    "        showlegend=False,\n",
    "        height=350 * len(attributes), # Dynamic height\n",
    "        width=1200,\n",
    "        margin=dict(l=120, r=20, t=80, b=20),\n",
    "        plot_bgcolor='white'\n",
    "    )\n",
    "\n",
    "    # Style axes for all subplots\n",
    "    fig.update_xaxes(\n",
    "        showticklabels=False,\n",
    "        showgrid=False,\n",
    "        zeroline=False\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        range=[0, 110],\n",
    "        showgrid=True,\n",
    "        gridcolor='#e5e5e5',\n",
    "        tickvals=[0, 25, 50, 75, 100],\n",
    "        ticktext=['0%', '25%', '50%', '75%', '100%'],\n",
    "        zeroline=True,\n",
    "        zerolinecolor='#333',\n",
    "        zerolinewidth=1.5\n",
    "    )\n",
    "    # Hide all Y-axis ticks and labels except for the first column\n",
    "    for c in range(2, len(metrics) + 1):\n",
    "        fig.update_yaxes(showticklabels=False, col=c)\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # ----------------- SUMMARY STATISTICS (UNCHANGED) ---------------------------\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"PRODUCT PORTFOLIO ANALYSIS SUMMARY (SORTED BY VARIABILITY)\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"\\nOVERALL METRICS:\")\n",
    "    for metric_name, metric_col in metrics.items():\n",
    "        total = sku_enriched[metric_col].sum()\n",
    "        print(f\"  • {metric_name}: {format_number(total)}\")\n",
    "    print(f\"\\nATTRIBUTE VARIABILITY RANKING:\")\n",
    "    for i, (attr, var_score) in enumerate(sorted_attributes, 1):\n",
    "        print(f\"  {i}. {attr.replace('_', ' ').title()}: {var_score:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataset(sku_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔄 Creating sales_long DataFrame from historical_sales...\")\n",
    "\n",
    "# Function to extract daily sales records from historical_sales\n",
    "def extract_sales_records(sku_enriched):\n",
    "    \"\"\"Extract individual sales records from historical_sales into long format\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    for idx, row in sku_enriched.iterrows():\n",
    "        sku = row['sku']\n",
    "        historical_sales = row['historical_sales']\n",
    "        \n",
    "        # Skip if no historical sales data\n",
    "        if not isinstance(historical_sales, (list, np.ndarray)) or len(historical_sales) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Extract each sales record\n",
    "        for sale_record in historical_sales:\n",
    "            if isinstance(sale_record, dict) and 'date' in sale_record and 'quantity' in sale_record:\n",
    "                # Safely get unit_price and quantity, ensuring no None values\n",
    "                quantity = sale_record.get('quantity', 0)\n",
    "                unit_price = sale_record.get('unit_price', 0)\n",
    "                \n",
    "                # Convert None values to 0\n",
    "                if quantity is None:\n",
    "                    quantity = 0\n",
    "                if unit_price is None:\n",
    "                    unit_price = 0\n",
    "                \n",
    "                # Parse date more carefully\n",
    "                try:\n",
    "                    if isinstance(sale_record['date'], str):\n",
    "                        # Try different date formats\n",
    "                        for date_format in ['%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y', '%d/%m/%Y']:\n",
    "                            try:\n",
    "                                parsed_date = pd.to_datetime(sale_record['date'], format=date_format)\n",
    "                                break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                        else:\n",
    "                            # If no format works, use pandas parser\n",
    "                            parsed_date = pd.to_datetime(sale_record['date'])\n",
    "                    else:\n",
    "                        parsed_date = pd.to_datetime(sale_record['date'])\n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"Warning: Could not parse date {sale_record['date']} for SKU {sku}\")\n",
    "                    continue\n",
    "                \n",
    "                # Create record with all attributes from the SKU row\n",
    "                record = {\n",
    "                    'sku': sku,\n",
    "                    'date': parsed_date,\n",
    "                    'quantity': quantity,\n",
    "                    'unit_price': unit_price,\n",
    "                    'revenue': quantity * unit_price\n",
    "                }\n",
    "                \n",
    "                # Add all other columns from sku_enriched row to preserve attributes\n",
    "                for col in sku_enriched.columns:\n",
    "                    if col not in ['sku', 'historical_sales']:\n",
    "                        record[col] = row[col]\n",
    "                \n",
    "                records.append(record)\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Extract records and create sales_long DataFrame\n",
    "records = extract_sales_records(sku_enriched)\n",
    "sales_long = pd.DataFrame(records)\n",
    "\n",
    "sales_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_dataset(sales_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎛️ INTERACTIVE SALES DASHBOARD - Comprehensive Analysis Interface\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if sales_long has data\n",
    "if not sales_long.empty:\n",
    "    # Ensure date column is properly formatted\n",
    "    sales_long['date'] = pd.to_datetime(sales_long['date'])\n",
    "    sales_long = sales_long.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"✅ Created sales_long DataFrame with {len(sales_long)} sales records from {sales_long['sku'].nunique()} SKUs\")\n",
    "    print(f\"📋 Available columns in sales_long: {list(sales_long.columns)}\")\n",
    "    \n",
    "    # Data quality checks\n",
    "    print(f\"\\n📊 Data Quality Checks:\")\n",
    "    print(f\"  • Date range: {sales_long['date'].min().strftime('%Y-%m-%d')} to {sales_long['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"  • Total sales records: {len(sales_long):,}\")\n",
    "    print(f\"  • Unique SKUs: {sales_long['sku'].nunique()}\")\n",
    "    print(f\"  • Total quantity sold: {sales_long['quantity'].sum():,.0f}\")\n",
    "    print(f\"  • Total revenue: ${sales_long['revenue'].sum():,.2f}\")\n",
    "    print(f\"  • Records with zero quantity: {(sales_long['quantity'] == 0).sum()}\")\n",
    "    print(f\"  • Records with missing dates: {sales_long['date'].isna().sum()}\")\n",
    "    \n",
    "    # Show sample of recent data\n",
    "    print(f\"\\n📝 Sample of most recent sales:\")\n",
    "    recent_sample = sales_long.nlargest(5, 'date')[['sku', 'date', 'quantity', 'unit_price', 'revenue']]\n",
    "    for _, row in recent_sample.iterrows():\n",
    "        print(f\"  • {row['sku']}: {row['date'].strftime('%Y-%m-%d')} - {row['quantity']} units @ ${row['unit_price']:.2f} = ${row['revenue']:.2f}\")\n",
    "    \n",
    "    print(f\"\\n🏷️ Available attribute columns for breakdown: {attr_cols}\")\n",
    "    \n",
    "    # Create Interactive Dashboard Class\n",
    "    class SalesDashboard:\n",
    "        def __init__(self, sales_data, attribute_columns):\n",
    "            self.sales_data = sales_data\n",
    "            self.attr_cols = attribute_columns\n",
    "            self.output_widget = widgets.Output()\n",
    "            \n",
    "            # Create control widgets\n",
    "            self.create_controls()\n",
    "            self.create_dashboard_layout()\n",
    "            \n",
    "        def create_controls(self):\n",
    "            \"\"\"Create all control widgets\"\"\"\n",
    "            # Attribute selection dropdown - only Overall and attr_cols\n",
    "            dropdown_options = [('Overall (No Breakdown)', 'none')]\n",
    "            for col in self.attr_cols:\n",
    "                dropdown_options.append((col.replace('_', ' ').title(), col))\n",
    "            \n",
    "            self.attr_dropdown = widgets.Dropdown(\n",
    "                options=dropdown_options,\n",
    "                value='none',\n",
    "                description='Breakdown by:',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='300px')\n",
    "            )\n",
    "            \n",
    "            # Metric selection dropdown\n",
    "            self.metric_dropdown = widgets.Dropdown(\n",
    "                options=[\n",
    "                    ('Total Units Sold', 'quantity'),\n",
    "                    ('Total Revenue', 'revenue'),\n",
    "                    ('Percentage of Units', 'quantity_pct'),\n",
    "                    ('Percentage of Revenue', 'revenue_pct')\n",
    "                ],\n",
    "                value='quantity',\n",
    "                description='Metric:',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='300px')\n",
    "            )\n",
    "            \n",
    "            # Rolling average slider\n",
    "            self.rolling_slider = widgets.IntSlider(\n",
    "                value=1,\n",
    "                min=1,\n",
    "                max=30,\n",
    "                step=1,\n",
    "                description='Rolling Avg (days):',\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='400px')\n",
    "            )\n",
    "            \n",
    "            # Update button\n",
    "            self.update_button = widgets.Button(\n",
    "                description='🔄 Update Chart',\n",
    "                button_style='primary',\n",
    "                layout=widgets.Layout(width='150px')\n",
    "            )\n",
    "            \n",
    "            # Bind events\n",
    "            self.update_button.on_click(self.update_charts)\n",
    "            \n",
    "        def create_dashboard_layout(self):\n",
    "            \"\"\"Create the dashboard layout\"\"\"\n",
    "            # Control panel\n",
    "            controls_box = widgets.VBox([\n",
    "                widgets.HTML(\"<h3>📊 Sales Dashboard Controls</h3>\"),\n",
    "                widgets.HBox([self.attr_dropdown, self.metric_dropdown]),\n",
    "                self.rolling_slider,\n",
    "                self.update_button,\n",
    "                widgets.HTML(\"<hr>\")\n",
    "            ])\n",
    "            \n",
    "            # Main dashboard\n",
    "            self.dashboard = widgets.VBox([\n",
    "                controls_box,\n",
    "                self.output_widget\n",
    "            ])\n",
    "            \n",
    "        def prepare_data(self, attr_name, metric_name, rolling_days):\n",
    "            \"\"\"Prepare data based on selected parameters - only use attr_cols\"\"\"\n",
    "            data = self.sales_data.copy()\n",
    "            \n",
    "            if attr_name == 'none':\n",
    "                # Overall data - no breakdown\n",
    "                if metric_name in ['quantity', 'revenue']:\n",
    "                    daily_data = data.groupby('date')[metric_name].sum().reset_index()\n",
    "                    if rolling_days > 1:\n",
    "                        daily_data[f'{metric_name}_smooth'] = daily_data[metric_name].rolling(window=rolling_days, center=True).mean()\n",
    "                        return daily_data, None, f\"Total {metric_name.title()}\"\n",
    "                    return daily_data, None, f\"Daily {metric_name.title()}\"\n",
    "                else:\n",
    "                    # For percentage, show 100% line\n",
    "                    daily_data = data.groupby('date')['quantity'].sum().reset_index()\n",
    "                    daily_data['pct'] = 100.0\n",
    "                    return daily_data, None, \"100% (No Breakdown)\"\n",
    "            else:\n",
    "                # Breakdown by attribute - ONLY if it's in attr_cols\n",
    "                if attr_name not in self.attr_cols:\n",
    "                    raise ValueError(f\"Invalid breakdown attribute: {attr_name}. Must be one of {self.attr_cols}\")\n",
    "                \n",
    "                # Check if the attribute column exists in the data\n",
    "                if attr_name not in data.columns:\n",
    "                    raise ValueError(f\"Column '{attr_name}' not found in sales data\")\n",
    "                \n",
    "                if metric_name in ['quantity_pct', 'revenue_pct']:\n",
    "                    # Percentage breakdown\n",
    "                    base_metric = metric_name.replace('_pct', '')\n",
    "                    daily_totals = data.groupby('date')[base_metric].sum()\n",
    "                    daily_breakdown = data.groupby(['date', attr_name])[base_metric].sum().unstack(fill_value=0)\n",
    "                    \n",
    "                    # Calculate percentages\n",
    "                    pct_breakdown = daily_breakdown.div(daily_totals, axis=0) * 100\n",
    "                    \n",
    "                    if rolling_days > 1:\n",
    "                        pct_breakdown = pct_breakdown.rolling(window=rolling_days, center=True).mean()\n",
    "                    \n",
    "                    return pct_breakdown, daily_breakdown, f\"Percentage of {base_metric.title()}\"\n",
    "                else:\n",
    "                    # Absolute values breakdown\n",
    "                    daily_breakdown = data.groupby(['date', attr_name])[metric_name].sum().unstack(fill_value=0)\n",
    "                    \n",
    "                    if rolling_days > 1:\n",
    "                        daily_breakdown = daily_breakdown.rolling(window=rolling_days, center=True).mean()\n",
    "                    \n",
    "                    # Also calculate totals\n",
    "                    daily_totals = daily_breakdown.sum(axis=1)\n",
    "                    \n",
    "                    return daily_breakdown, daily_totals, f\"Total {metric_name.title()}\"\n",
    "                    \n",
    "        def create_chart(self, data, totals, attr_name, metric_name, rolling_days):\n",
    "            \"\"\"Create the appropriate chart based on parameters\"\"\"\n",
    "            if attr_name == 'none':\n",
    "                # Single line chart\n",
    "                fig = go.Figure()\n",
    "                \n",
    "                metric_col = f'{metric_name}_smooth' if rolling_days > 1 and f'{metric_name}_smooth' in data.columns else metric_name\n",
    "                \n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=data['date'],\n",
    "                    y=data[metric_col] if metric_col in data.columns else data['pct'],\n",
    "                    mode='lines',\n",
    "                    name=f'{metric_name.title()}',\n",
    "                    line=dict(color='#2E86AB', width=2),\n",
    "                    fill='tonexty'\n",
    "                ))\n",
    "                \n",
    "                title = f\"📈 {metric_name.replace('_', ' ').title()}\"\n",
    "                if rolling_days > 1:\n",
    "                    title += f\" ({rolling_days}-day rolling average)\"\n",
    "                    \n",
    "            elif metric_name.endswith('_pct'):\n",
    "                # Stacked area chart for percentages\n",
    "                fig = go.Figure()\n",
    "                \n",
    "                colors = plotly.colors.qualitative.Set3[:len(data.columns)]\n",
    "                \n",
    "                for i, category in enumerate(data.columns):\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=data.index,\n",
    "                        y=data[category],\n",
    "                        mode='lines',\n",
    "                        name=str(category),\n",
    "                        stackgroup='one',\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=colors[i % len(colors)]\n",
    "                    ))\n",
    "                \n",
    "                fig.update_layout(\n",
    "                    yaxis=dict(title='Percentage (%)', range=[0, 100]),\n",
    "                    hovermode='x unified'\n",
    "                )\n",
    "                \n",
    "                title = f\"📊 Percentage Breakdown by {attr_name.replace('_', ' ').title()}\"\n",
    "                if rolling_days > 1:\n",
    "                    title += f\" ({rolling_days}-day rolling average)\"\n",
    "                    \n",
    "            else:\n",
    "                # Stacked area chart for absolute values\n",
    "                fig = go.Figure()\n",
    "                \n",
    "                colors = plotly.colors.qualitative.Set3[:len(data.columns)]\n",
    "                \n",
    "                for i, category in enumerate(data.columns):\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=data.index,\n",
    "                        y=data[category],\n",
    "                        mode='lines',\n",
    "                        name=str(category),\n",
    "                        stackgroup='one',\n",
    "                        line=dict(width=0),\n",
    "                        fillcolor=colors[i % len(colors)]\n",
    "                    ))\n",
    "                \n",
    "                # Add total line if available\n",
    "                if totals is not None:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=data.index,\n",
    "                        y=totals,\n",
    "                        mode='lines',\n",
    "                        name='Total',\n",
    "                        line=dict(color='black', width=2, dash='dash')\n",
    "                    ))\n",
    "                \n",
    "                title = f\"📈 {metric_name.title()} Breakdown by {attr_name.replace('_', ' ').title()}\"\n",
    "                if rolling_days > 1:\n",
    "                    title += f\" ({rolling_days}-day rolling average)\"\n",
    "            \n",
    "            # Common layout settings\n",
    "            fig.update_layout(\n",
    "                title=title,\n",
    "                xaxis_title='Date',\n",
    "                width=1000,\n",
    "                height=600,\n",
    "                showlegend=True,\n",
    "                legend=dict(orientation=\"v\", yanchor=\"top\", y=1, xanchor=\"left\", x=1.02)\n",
    "            )\n",
    "            \n",
    "            return fig\n",
    "            \n",
    "        def update_charts(self, button_click=None):\n",
    "            \"\"\"Update charts based on current selections\"\"\"\n",
    "            with self.output_widget:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                try:\n",
    "                    # Get current selections\n",
    "                    attr_name = self.attr_dropdown.value\n",
    "                    metric_name = self.metric_dropdown.value\n",
    "                    rolling_days = self.rolling_slider.value\n",
    "                    \n",
    "                    print(\"🔄 Updating dashboard...\")\n",
    "                    print(\"=\" * 80)\n",
    "                    print(f\"🎯 Selected breakdown: {attr_name if attr_name != 'none' else 'Overall'}\")\n",
    "                    print(f\"📊 Available attr_cols: {self.attr_cols}\")\n",
    "                    \n",
    "                    # Prepare data\n",
    "                    main_data, totals, chart_title = self.prepare_data(attr_name, metric_name, rolling_days)\n",
    "                    \n",
    "                    # Create and display chart\n",
    "                    fig = self.create_chart(main_data, totals, attr_name, metric_name, rolling_days)\n",
    "                    fig.show()\n",
    "                    \n",
    "                    # Print summary information\n",
    "                    print(\"=\" * 80)\n",
    "                    rolling_text = f\" with {rolling_days}-day rolling average\" if rolling_days > 1 else \"\"\n",
    "                    breakdown_text = f\"by {attr_name.replace('_', ' ').title()}\" if attr_name != 'none' else \"Overall\"\n",
    "                    print(f\"🎯 Chart: {breakdown_text} - {metric_name.replace('_', ' ').title()}{rolling_text}\")\n",
    "                    \n",
    "                    # Data summary - Fix date extraction logic\n",
    "                    if attr_name == 'none':\n",
    "                        # For overall data, use 'date' column\n",
    "                        data_start = main_data['date'].min().strftime('%Y-%m-%d')\n",
    "                        data_end = main_data['date'].max().strftime('%Y-%m-%d')\n",
    "                        total_days = len(main_data)\n",
    "                    else:\n",
    "                        # For breakdown data, use index (which are dates)\n",
    "                        if hasattr(main_data.index, 'min') and hasattr(main_data.index.min(), 'strftime'):\n",
    "                            data_start = main_data.index.min().strftime('%Y-%m-%d')\n",
    "                            data_end = main_data.index.max().strftime('%Y-%m-%d')\n",
    "                        else:\n",
    "                            # Fallback if index is not datetime\n",
    "                            data_start = str(main_data.index.min())\n",
    "                            data_end = str(main_data.index.max())\n",
    "                        total_days = len(main_data.index)\n",
    "                        \n",
    "                    print(f\"📅 Time Range: {data_start} to {data_end} ({total_days} days)\")\n",
    "                    print(f\"📊 Breakdown: {breakdown_text}\")\n",
    "                    print(f\"📈 Metric: {metric_name.replace('_', ' ').title()}\")\n",
    "                    \n",
    "                    # Show category breakdown if applicable\n",
    "                    if attr_name != 'none' and hasattr(main_data, 'columns'):\n",
    "                        categories = main_data.columns.tolist()\n",
    "                        print(f\"🏷️  Categories ({len(categories)}): {', '.join(str(c) for c in categories)}\")\n",
    "                        print(f\"🔍 Breakdown attribute: {attr_name} (from attr_cols)\")\n",
    "                    \n",
    "                    print(\"=\" * 80)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error creating charts: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "        \n",
    "        def display(self):\n",
    "            \"\"\"Display the dashboard\"\"\"\n",
    "            display(self.dashboard)\n",
    "            # Auto-generate initial chart\n",
    "            self.update_charts(None)\n",
    "\n",
    "    # Create and display the dashboard\n",
    "    if attr_cols:\n",
    "        print(\"\\n🚀 Creating Interactive Sales Dashboard...\")\n",
    "        print(f\"📋 Dashboard breakdown options:\")\n",
    "        print(f\"   • Overall (no breakdown)\")\n",
    "        for col in attr_cols:\n",
    "            print(f\"   • {col.replace('_', ' ').title()}\")\n",
    "        print(f\"\\n🔒 Only these {len(attr_cols)} attribute columns will be available for breakdown\")\n",
    "        dashboard = SalesDashboard(sales_long, attr_cols)\n",
    "        dashboard.display()\n",
    "    else:\n",
    "        print(\"❌ No suitable attribute columns found for dashboard creation.\")\n",
    "        print(\"   Available columns:\", list(sales_long.columns))\n",
    "        print(\"   Base columns excluded:\", base_cols)\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No data available for dashboard creation - sales_long DataFrame is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Private Credit & Product Analysis Visualizations\n",
    "\n",
    "This section provides comprehensive visualizations for private credit analysis and product performance evaluation, focusing on sales trends, revenue patterns, pricing dynamics, inventory metrics, and velocity analysis. Created by Claude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sales Performance Analysis - Time Series Overview\n",
    "print(\"🏪 SALES PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create figure with subplots for sales metrics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Sales Performance Analysis Across Time Periods', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Sales volume by time period\n",
    "sales_periods = ['sales_last_30d', 'sales_last_90d', 'sales_last_180d', 'sales_last_360d', 'total_sales']\n",
    "sales_values = [sku_enriched[col].sum() for col in sales_periods]\n",
    "time_labels = ['30 Days', '90 Days', '180 Days', '360 Days', 'Total']\n",
    "\n",
    "ax1.bar(time_labels, sales_values, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#8B5A2B'])\n",
    "ax1.set_title('Total Sales Volume by Time Period', fontweight='bold')\n",
    "ax1.set_ylabel('Total Sales Units')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(sales_values):\n",
    "    ax1.text(i, v + max(sales_values)*0.01, f'{v:,.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Distribution of sales performance (30-day focus)\n",
    "ax2.hist(sku_enriched['sales_last_30d'], bins=20, color='#2E86AB', alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Distribution of 30-Day Sales Performance', fontweight='bold')\n",
    "ax2.set_xlabel('Sales Units (Last 30 Days)')\n",
    "ax2.set_ylabel('Number of Products')\n",
    "ax2.axvline(sku_enriched['sales_last_30d'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {sku_enriched[\"sales_last_30d\"].mean():.1f}')\n",
    "ax2.legend()\n",
    "\n",
    "# Sales velocity comparison\n",
    "velocities = ['velocity_last_30d', 'velocity_last_90d', 'velocity_last_180d', 'velocity_last_360d']\n",
    "velocity_means = [sku_enriched[col].mean() for col in velocities]\n",
    "velocity_labels = ['30d Velocity', '90d Velocity', '180d Velocity', '360d Velocity']\n",
    "\n",
    "ax3.plot(velocity_labels, velocity_means, marker='o', linewidth=3, markersize=8, color='#C73E1D')\n",
    "ax3.fill_between(velocity_labels, velocity_means, alpha=0.3, color='#C73E1D')\n",
    "ax3.set_title('Average Sales Velocity Trends', fontweight='bold')\n",
    "ax3.set_ylabel('Units per Day')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Top performers vs bottom performers\n",
    "top_performers = sku_enriched.nlargest(10, 'sales_last_30d')[['name', 'sales_last_30d']].head(5)\n",
    "bottom_performers = sku_enriched.nsmallest(10, 'sales_last_30d')[['name', 'sales_last_30d']].head(5)\n",
    "\n",
    "y_pos = range(len(top_performers))\n",
    "ax4.barh(y_pos, top_performers['sales_last_30d'], color='#2E86AB', alpha=0.8)\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels([name[:20] + '...' if len(name) > 20 else name for name in top_performers['name']], fontsize=8)\n",
    "ax4.set_title('Top 5 Products by 30-Day Sales', fontweight='bold')\n",
    "ax4.set_xlabel('Sales Units (Last 30 Days)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key statistics\n",
    "print(f\"\\n📈 KEY SALES METRICS:\")\n",
    "print(f\"• Total sales across all products: {sku_enriched['total_sales'].sum():,} units\")\n",
    "print(f\"• Average 30-day sales per product: {sku_enriched['sales_last_30d'].mean():.1f} units\")\n",
    "print(f\"• Products with zero 30-day sales: {(sku_enriched['sales_last_30d'] == 0).sum()}\")\n",
    "print(f\"• Best performing product (30d): {sku_enriched.loc[sku_enriched['sales_last_30d'].idxmax(), 'name']} ({sku_enriched['sales_last_30d'].max()} units)\")\n",
    "print(f\"• Average sales velocity: {sku_enriched['velocity'].mean():.3f} units/day\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Revenue Analysis - Financial Performance Dashboard\n",
    "print(\"💰 REVENUE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comprehensive revenue dashboard\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = gridspec.GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Revenue trends across time periods\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "revenue_periods = ['revenue_last_30d', 'revenue_last_60d', 'revenue_last_90d', 'revenue_last_180d', 'revenue_last_360d', 'total_revenue']\n",
    "revenue_values = [sku_enriched[col].sum() for col in revenue_periods]\n",
    "revenue_labels = ['30 Days', '60 Days', '90 Days', '180 Days', '360 Days', 'Total']\n",
    "\n",
    "bars = ax1.bar(revenue_labels, revenue_values, color=['#1B4332', '#2D6A4F', '#40916C', '#52B788', '#74C69D', '#95D5B2'])\n",
    "ax1.set_title('Total Revenue by Time Period', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Revenue ($)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, revenue_values):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + max(revenue_values)*0.01, \n",
    "             f'${value:,.0f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Daily revenue trends\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "daily_revenues = ['avg_daily_revenue_30d', 'avg_daily_revenue_60d', 'avg_daily_revenue_90d', \n",
    "                  'avg_daily_revenue_180d', 'avg_daily_revenue_360d']\n",
    "daily_values = [sku_enriched[col].mean() for col in daily_revenues]\n",
    "daily_labels = ['30d', '60d', '90d', '180d', '360d']\n",
    "\n",
    "ax2.plot(daily_labels, daily_values, marker='o', linewidth=3, markersize=8, color='#2D6A4F')\n",
    "ax2.fill_between(daily_labels, daily_values, alpha=0.3, color='#2D6A4F')\n",
    "ax2.set_title('Avg Daily Revenue Trends', fontweight='bold')\n",
    "ax2.set_ylabel('Daily Revenue ($)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Revenue distribution (last 30 days)\n",
    "ax3 = fig.add_subplot(gs[1, 0])\n",
    "ax3.hist(sku_enriched['revenue_last_30d'], bins=25, color='#40916C', alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('30-Day Revenue Distribution', fontweight='bold')\n",
    "ax3.set_xlabel('Revenue ($)')\n",
    "ax3.set_ylabel('Number of Products')\n",
    "ax3.axvline(sku_enriched['revenue_last_30d'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: ${sku_enriched[\"revenue_last_30d\"].mean():.0f}')\n",
    "ax3.legend()\n",
    "\n",
    "# Revenue concentration (Pareto analysis)\n",
    "ax4 = fig.add_subplot(gs[1, 1])\n",
    "sorted_revenue = sku_enriched['total_revenue'].sort_values(ascending=False)\n",
    "cumulative_pct = (sorted_revenue.cumsum() / sorted_revenue.sum() * 100)\n",
    "ax4.plot(range(1, len(cumulative_pct) + 1), cumulative_pct, color='#1B4332', linewidth=2)\n",
    "ax4.axhline(y=80, color='red', linestyle='--', label='80% Revenue')\n",
    "ax4.axvline(x=len(cumulative_pct) * 0.2, color='orange', linestyle='--', label='20% Products')\n",
    "ax4.set_title('Revenue Concentration (Pareto)', fontweight='bold')\n",
    "ax4.set_xlabel('Product Rank')\n",
    "ax4.set_ylabel('Cumulative Revenue %')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Top revenue generators\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "top_revenue = sku_enriched.nlargest(8, 'revenue_last_30d')[['name', 'revenue_last_30d']]\n",
    "y_pos = range(len(top_revenue))\n",
    "bars = ax5.barh(y_pos, top_revenue['revenue_last_30d'], color='#52B788')\n",
    "ax5.set_yticks(y_pos)\n",
    "ax5.set_yticklabels([name[:15] + '...' if len(name) > 15 else name for name in top_revenue['name']], fontsize=9)\n",
    "ax5.set_title('Top Revenue Products (30d)', fontweight='bold')\n",
    "ax5.set_xlabel('Revenue ($)')\n",
    "\n",
    "# Revenue vs Sales correlation\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "ax6.scatter(sku_enriched['sales_last_30d'], sku_enriched['revenue_last_30d'], \n",
    "           alpha=0.6, color='#74C69D', edgecolors='black', linewidth=0.5)\n",
    "ax6.set_title('Revenue vs Sales Correlation', fontweight='bold')\n",
    "ax6.set_xlabel('Sales Units (30d)')\n",
    "ax6.set_ylabel('Revenue (30d)')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "correlation = sku_enriched['sales_last_30d'].corr(sku_enriched['revenue_last_30d'])\n",
    "ax6.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=ax6.transAxes, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8), fontweight='bold')\n",
    "\n",
    "# Average unit price analysis\n",
    "ax7 = fig.add_subplot(gs[2, 1])\n",
    "# Use available price columns and create 30d average from recent sales data\n",
    "current_avg_price = sku_enriched['unit_price'].mean()\n",
    "# Calculate weighted average price from recent revenue/sales ratio where sales > 0\n",
    "recent_avg_price = (sku_enriched[sku_enriched['sales_last_30d'] > 0]['revenue_last_30d'] / \n",
    "                   sku_enriched[sku_enriched['sales_last_30d'] > 0]['sales_last_30d']).mean()\n",
    "\n",
    "price_comparison = [current_avg_price, recent_avg_price]\n",
    "price_labels = ['Current Avg Price', '30-Day Avg Price']\n",
    "bars = ax7.bar(price_labels, price_comparison, color=['#2D6A4F', '#52B788'])\n",
    "ax7.set_title('Average Unit Price Comparison', fontweight='bold')\n",
    "ax7.set_ylabel('Price ($)')\n",
    "for bar, value in zip(bars, price_comparison):\n",
    "    height = bar.get_height()\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2., height + max(price_comparison)*0.01, \n",
    "             f'${value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Price range analysis\n",
    "ax8 = fig.add_subplot(gs[2, 2])\n",
    "price_ranges = sku_enriched['max_unit_price'] - sku_enriched['min_unit_price']\n",
    "ax8.hist(price_ranges, bins=20, color='#95D5B2', alpha=0.7, edgecolor='black')\n",
    "ax8.set_title('Price Range Distribution', fontweight='bold')\n",
    "ax8.set_xlabel('Price Range ($)')\n",
    "ax8.set_ylabel('Number of Products')\n",
    "ax8.axvline(price_ranges.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: ${price_ranges.mean():.2f}')\n",
    "ax8.legend()\n",
    "\n",
    "plt.suptitle('Revenue Analysis Dashboard', fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Print revenue insights\n",
    "print(f\"\\n💸 KEY REVENUE INSIGHTS:\")\n",
    "print(f\"• Total revenue: ${sku_enriched['total_revenue'].sum():,.2f}\")\n",
    "print(f\"• 30-day revenue: ${sku_enriched['revenue_last_30d'].sum():,.2f}\")\n",
    "print(f\"• Average revenue per product (30d): ${sku_enriched['revenue_last_30d'].mean():,.2f}\")\n",
    "print(f\"• Top revenue product: {sku_enriched.loc[sku_enriched['revenue_last_30d'].idxmax(), 'name']}\")\n",
    "print(f\"• Revenue concentration: Top 20% products generate {cumulative_pct.iloc[int(len(cumulative_pct)*0.2)-1]:.1f}% of revenue\")\n",
    "print(f\"• Average unit price: ${sku_enriched['unit_price'].mean():.2f}\")\n",
    "print(f\"• Price range (avg): ${price_ranges.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Cash Flow & Working Capital Analysis\n",
    "print(\"💰 CASH FLOW & WORKING CAPITAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate working capital metrics\n",
    "def calculate_working_capital_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate key working capital metrics for credit assessment\n",
    "    \"\"\"\n",
    "    # Days Sales Outstanding (Revenue tied up in inventory)\n",
    "    df['inventory_value'] = df['current_inventory'] * df['avg_unit_price']\n",
    "    df['daily_revenue'] = df['revenue_last_30d'] / 30\n",
    "    df['days_sales_outstanding'] = df['inventory_value'] / df['daily_revenue'].replace(0, np.nan)\n",
    "    \n",
    "    # Working capital efficiency\n",
    "    df['revenue_per_inventory_dollar'] = df['revenue_last_30d'] / df['inventory_value'].replace(0, np.nan)\n",
    "    \n",
    "    # Cash generation velocity\n",
    "    df['cash_velocity'] = df['revenue_last_30d'] / (df['inventory_value'] + 1)  # Add 1 to avoid division by zero\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply working capital calculations\n",
    "sku_working_capital = calculate_working_capital_metrics(sku_enriched.copy())\n",
    "\n",
    "# Create working capital dashboard\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Working Capital & Cash Flow Analysis - Private Credit Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Days Sales Outstanding Distribution\n",
    "dso_data = sku_working_capital['days_sales_outstanding'].dropna()\n",
    "dso_data = dso_data[dso_data < dso_data.quantile(0.95)]  # Remove outliers\n",
    "ax1.hist(dso_data, bins=30, color='#3498DB', alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Days Sales Outstanding Distribution', fontweight='bold')\n",
    "ax1.set_xlabel('Days')\n",
    "ax1.set_ylabel('Number of Products')\n",
    "ax1.axvline(dso_data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {dso_data.mean():.1f} days')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Revenue per Inventory Dollar\n",
    "rev_per_inv = sku_working_capital['revenue_per_inventory_dollar'].dropna()\n",
    "rev_per_inv = rev_per_inv[rev_per_inv < rev_per_inv.quantile(0.95)]  # Remove outliers\n",
    "ax2.hist(rev_per_inv, bins=30, color='#2ECC71', alpha=0.7, edgecolor='black')\n",
    "ax2.set_title('Revenue per Inventory Dollar Efficiency', fontweight='bold')\n",
    "ax2.set_xlabel('Revenue per $ of Inventory')\n",
    "ax2.set_ylabel('Number of Products')\n",
    "ax2.axvline(rev_per_inv.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${rev_per_inv.mean():.2f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Cash Velocity vs Sales Performance\n",
    "ax3.scatter(sku_working_capital['cash_velocity'], sku_working_capital['sales_last_30d'], \n",
    "           alpha=0.6, color='#E74C3C', s=50)\n",
    "ax3.set_title('Cash Velocity vs Sales Performance', fontweight='bold')\n",
    "ax3.set_xlabel('Cash Velocity (Revenue/Inventory Value)')\n",
    "ax3.set_ylabel('Sales (30d)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Working Capital Risk Matrix\n",
    "# High inventory value + Low revenue = High risk\n",
    "inv_value_norm = (sku_working_capital['inventory_value'] - sku_working_capital['inventory_value'].min()) / (sku_working_capital['inventory_value'].max() - sku_working_capital['inventory_value'].min())\n",
    "revenue_norm = (sku_working_capital['revenue_last_30d'] - sku_working_capital['revenue_last_30d'].min()) / (sku_working_capital['revenue_last_30d'].max() - sku_working_capital['revenue_last_30d'].min())\n",
    "\n",
    "scatter = ax4.scatter(inv_value_norm, revenue_norm, \n",
    "                     c=sku_working_capital['days_sales_outstanding'], \n",
    "                     cmap='RdYlBu_r', alpha=0.7, s=60)\n",
    "ax4.set_title('Working Capital Risk Matrix', fontweight='bold')\n",
    "ax4.set_xlabel('Inventory Value (Normalized)')\n",
    "ax4.set_ylabel('Revenue Performance (Normalized)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add risk quadrants\n",
    "ax4.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.axvline(x=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.text(0.75, 0.25, 'High Risk\\n(High Inv, Low Rev)', ha='center', va='center', \n",
    "         bbox=dict(boxstyle='round', facecolor='red', alpha=0.3))\n",
    "ax4.text(0.25, 0.75, 'Low Risk\\n(Low Inv, High Rev)', ha='center', va='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='green', alpha=0.3))\n",
    "\n",
    "plt.colorbar(scatter, ax=ax4, label='Days Sales Outstanding')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print working capital insights\n",
    "print(\"\\n💰 WORKING CAPITAL INSIGHTS:\")\n",
    "print(f\"• Average Days Sales Outstanding: {dso_data.mean():.1f} days\")\n",
    "print(f\"• Average Revenue per Inventory $: ${rev_per_inv.mean():.2f}\")\n",
    "print(f\"• Total inventory value: ${sku_working_capital['inventory_value'].sum():,.0f}\")\n",
    "print(f\"• High risk products (top 25% DSO): {(sku_working_capital['days_sales_outstanding'] > sku_working_capital['days_sales_outstanding'].quantile(0.75)).sum()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
